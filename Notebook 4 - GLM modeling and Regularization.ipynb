{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os as os\n",
    "import requests\n",
    "import io\n",
    "import urllib.request\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.io import arff\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\JM025575\\\\Predictive Models Class\\\\Week 3\\\\code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM025575\\Predictive Models Class\\data\n"
     ]
    }
   ],
   "source": [
    "cd /Users/JM025575/Predictive Models Class/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type float64\n",
      "Shape of Data (97, 9)\n",
      "Colums Names Index(['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45',\n",
      "       'lpsa'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lcavol</th>\n",
       "      <th>lweight</th>\n",
       "      <th>age</th>\n",
       "      <th>lbph</th>\n",
       "      <th>svi</th>\n",
       "      <th>lcp</th>\n",
       "      <th>gleason</th>\n",
       "      <th>pgg45</th>\n",
       "      <th>lpsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.579818</td>\n",
       "      <td>2.769459</td>\n",
       "      <td>50</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.430783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.994252</td>\n",
       "      <td>3.319626</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.510826</td>\n",
       "      <td>2.691243</td>\n",
       "      <td>74</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.162519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.203973</td>\n",
       "      <td>3.282789</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.751416</td>\n",
       "      <td>3.432373</td>\n",
       "      <td>62</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.371564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.049822</td>\n",
       "      <td>3.228826</td>\n",
       "      <td>50</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.765468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.737164</td>\n",
       "      <td>3.473518</td>\n",
       "      <td>64</td>\n",
       "      <td>0.615186</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.765468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>3.539509</td>\n",
       "      <td>58</td>\n",
       "      <td>1.536867</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.854415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.776529</td>\n",
       "      <td>3.539509</td>\n",
       "      <td>47</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.047319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.223144</td>\n",
       "      <td>3.244544</td>\n",
       "      <td>63</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.047319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lcavol   lweight  age      lbph  svi       lcp  gleason  pgg45      lpsa\n",
       "0 -0.579818  2.769459   50 -1.386294    0 -1.386294        6      0 -0.430783\n",
       "1 -0.994252  3.319626   58 -1.386294    0 -1.386294        6      0 -0.162519\n",
       "2 -0.510826  2.691243   74 -1.386294    0 -1.386294        7     20 -0.162519\n",
       "3 -1.203973  3.282789   58 -1.386294    0 -1.386294        6      0 -0.162519\n",
       "4  0.751416  3.432373   62 -1.386294    0 -1.386294        6      0  0.371564\n",
       "5 -1.049822  3.228826   50 -1.386294    0 -1.386294        6      0  0.765468\n",
       "6  0.737164  3.473518   64  0.615186    0 -1.386294        6      0  0.765468\n",
       "7  0.693147  3.539509   58  1.536867    0 -1.386294        6      0  0.854415\n",
       "8 -0.776529  3.539509   47 -1.386294    0 -1.386294        6      0  1.047319\n",
       "9  0.223144  3.244544   63 -1.386294    0 -1.386294        6      0  1.047319"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prostate = pd.read_csv(\"prostate.csv\", sep=\",\")\n",
    "#print type of object for target\n",
    "print(\"Data type\", prostate.lpsa.dtype)\n",
    "#Dimensions of dataset\n",
    "print(\"Shape of Data\", prostate.shape)\n",
    "#Colum names\n",
    "print(\"Colums Names\", prostate.columns)\n",
    "#See top few rows of dataset\n",
    "prostate.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45', 'lpsa']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prostate.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prostate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    97.000000\n",
       "mean      2.478387\n",
       "std       1.154329\n",
       "min      -0.430783\n",
       "25%       1.731656\n",
       "50%       2.591516\n",
       "75%       3.056357\n",
       "max       5.582932\n",
       "Name: lpsa, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the descriptive statistics of the target variable\n",
    "prostate.lpsa.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97,)\n",
      "(97, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM025575\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \n",
      "C:\\Users\\JM025575\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lcavol</th>\n",
       "      <th>lweight</th>\n",
       "      <th>age</th>\n",
       "      <th>lbph</th>\n",
       "      <th>svi</th>\n",
       "      <th>lcp</th>\n",
       "      <th>gleason</th>\n",
       "      <th>pgg45</th>\n",
       "      <th>lpsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.579818</td>\n",
       "      <td>2.769459</td>\n",
       "      <td>50</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.430783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.994252</td>\n",
       "      <td>3.319626</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.510826</td>\n",
       "      <td>2.691243</td>\n",
       "      <td>74</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.162519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.203973</td>\n",
       "      <td>3.282789</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.751416</td>\n",
       "      <td>3.432373</td>\n",
       "      <td>62</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.371564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lcavol   lweight  age      lbph  svi       lcp  gleason  pgg45      lpsa\n",
       "0 -0.579818  2.769459   50 -1.386294    0 -1.386294        6      0 -0.430783\n",
       "1 -0.994252  3.319626   58 -1.386294    0 -1.386294        6      0 -0.162519\n",
       "2 -0.510826  2.691243   74 -1.386294    0 -1.386294        7     20 -0.162519\n",
       "3 -1.203973  3.282789   58 -1.386294    0 -1.386294        6      0 -0.162519\n",
       "4  0.751416  3.432373   62 -1.386294    0 -1.386294        6      0  0.371564"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define the features and target, lpsa\n",
    "prostate.target=prostate.lpsa\n",
    "prostate.features=prostate.drop(['lpsa'], axis=1)                         \n",
    "print(prostate.target.shape)\n",
    "print(prostate.features.shape)\n",
    "prostate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression package\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Ridge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Coef 0.7682943732632037 [ 0.58397689  0.43665382 -0.01896769  0.10638469  0.68842174 -0.08707084\n",
      "  0.03692423  0.00465442]\n",
      "MSE 0.45596067462452833\n"
     ]
    }
   ],
   "source": [
    "# fit a ridge regression model to the data\n",
    "model_RG = Ridge(alpha = 1.0)\n",
    "model_RG.fit(prostate.features, prostate.target)\n",
    "print(model_RG)\n",
    "# make predictions\n",
    "expected_RG = prostate.target\n",
    "predicted_RG= model_RG.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_RG.intercept_, model_RG.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_RG, predicted_RG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic Ridge Regression model has a MSE of 0.4559 which I will work to lower in my following model adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(np.transpose(model_RG.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=5.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Coef 1.0375988316496547 [ 0.56679975  0.37688704 -0.01683074  0.10564496  0.49540035 -0.03622384\n",
      "  0.0192159   0.00491179]\n",
      "MSE 0.4644668982849343\n"
     ]
    }
   ],
   "source": [
    "#Adjusting the alpha in my model from the default of 1 to a 5.\n",
    "# fit a ridge regression model to the data\n",
    "model_RG = Ridge(alpha = 5.0)\n",
    "model_RG.fit(prostate.features, prostate.target)\n",
    "print(model_RG)\n",
    "# make predictions\n",
    "expected_RG = prostate.target\n",
    "predicted_RG= model_RG.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_RG.intercept_, model_RG.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_RG, predicted_RG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My second Ridge Regression model I adjusted my alpha, or error, up to 5.0.  This caused my MSE to go up slightly to 0.46446."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Coef 0.7210733869670383 [ 0.58560575  0.44540971 -0.01929328  0.10667301  0.72504385 -0.09585295\n",
      "  0.04073685  0.00459521]\n",
      "MSE 0.45547461120347865\n"
     ]
    }
   ],
   "source": [
    "#Adjusting the alpha in my model from 5 to 0.5.\n",
    "# fit a ridge regression model to the data\n",
    "model_RG = Ridge(alpha = 0.5)\n",
    "model_RG.fit(prostate.features, prostate.target)\n",
    "print(model_RG)\n",
    "# make predictions\n",
    "expected_RG = prostate.target\n",
    "predicted_RG= model_RG.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_RG.intercept_, model_RG.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_RG, predicted_RG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My third Ridge Regression model, I adjusted my alpha down to 0.5 to see if that would cause my MSE to go back down.  It did cause it to go down although it is just slightly lower than the basic model at 0.4554."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search to narrow down alpha and Cross Validation 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'alpha': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over several parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"alpha\": [0.01, 0.001, 0.0001, 0.05, 0.005, 0.75, 0.0005, 0.9]}  #different penalties I am playing with\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(model_RG, param_grid=param_grid,n_jobs=-1, cv = 10)\n",
    "grid_search.fit(prostate.features, prostate.target)\n",
    "print(\"Best\", grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a grid search, I ran 8 different alpha values through my model.  I use the grid search because it is a more efficient way of trying to find what my most ideal alpha value is rather than plugging one in at a time and seeing how my MSE adjusts.  I also used cross validation with the grid search cross validating 10 times.  Cross validation allows my model to be run a specific number of times, 10 in this case, each time taking different samples of data to run the model against for my various alphas I am using in the grid search.  Cross validation is a good way to see if we are overfitting our model. My result was that my best alpha is 0.9 demonstrating that continuing to lower my alpha value is not always the best way to get the lowest MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Model with Best Alpha From Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=0.9, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Coef 0.7591778129829336 [ 0.58431758  0.43838111 -0.01903145  0.10643602  0.69542291 -0.08876642\n",
      "  0.03764465  0.00464333]\n",
      "MSE 0.4558435387636189\n"
     ]
    }
   ],
   "source": [
    "#Ridge model with alpha of 0.9\n",
    "# fit a ridge regression model to the data\n",
    "model_RG = Ridge(alpha = 0.9)\n",
    "model_RG.fit(prostate.features, prostate.target)\n",
    "print(model_RG)\n",
    "# make predictions\n",
    "expected_RG = prostate.target\n",
    "predicted_RG= model_RG.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_RG.intercept_, model_RG.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_RG, predicted_RG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My final Ridge model I used the 0.9 alpha that was considered my best alpha based on the grid search and cross validation I did in the previous step.  This produced a MSE of 0.4558.  Based purely on the MSE, I would say that my best Ridge Regression model is my 3rd one with the alpha of 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression-Least Absolute Shrinkage and Selection Operator\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LASSO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Coef 2.0879365603204247 [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01601424]\n",
      "MSE 1.0848110162374005\n"
     ]
    }
   ],
   "source": [
    "# fit a LASSO model to the data\n",
    "model_LAS = Lasso(alpha=1)\n",
    "model_LAS.fit(prostate.features, prostate.target)\n",
    "print(model_LAS)\n",
    "# make predictions\n",
    "expected_LAS = prostate.target\n",
    "predicted_LAS = model_LAS.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_LAS.intercept_,model_LAS.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_LAS, predicted_LAS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic Lasso Regression model produced a MSE of 1.0848, with the alpha set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Coef 1.0301298227422002 [ 0.57839375  0.4109548  -0.0174482   0.10309292  0.6346103  -0.06304874\n",
      "  0.          0.00498874]\n",
      "MSE 0.45793415168104235\n"
     ]
    }
   ],
   "source": [
    "# Adjust the alpha to 0.01 from 1.0\n",
    "model_LAS = Lasso(alpha=0.01)\n",
    "model_LAS.fit(prostate.features, prostate.target)\n",
    "print(model_LAS)\n",
    "# make predictions\n",
    "expected_LAS = prostate.target\n",
    "predicted_LAS = model_LAS.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_LAS.intercept_,model_LAS.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_LAS, predicted_LAS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my second Lasso model, I adjusted my alpha down to 0.01 which caused my MSE to drop all the way down to 0.4579."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Grid Search With 3 Different Grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'alpha': 0.05}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"alpha\": [0.05, 0.01, 0.005]}  #different penalties I am playing with\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(model_LAS, param_grid=param_grid,n_jobs=-1)\n",
    "grid_search.fit(prostate.features, prostate.target)\n",
    "print(\"Best\", grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my first grid, I am feeding my Lasso model 3 alphas, including the alpha I used in my previous model that saw my MSE drop from the original model.  I also gave it an alpha that is above and below that value to see for my second grid where I need to adjust to next.  For grid one, my best alpha was 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'alpha': 0.05}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"alpha\": [0.05, 0.005, 0.0005]}  #different penalties I am playing with\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(model_LAS, param_grid=param_grid,n_jobs=-1)\n",
    "grid_search.fit(prostate.features, prostate.target)\n",
    "print(\"Best\", grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my second grid, I again fed it 3 values including 0.05 from my last model.  This time, the grid search produced a result saying my best alpha is still 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"alpha\": [0.05, 0.1, 0.5]}  #different penalties I am playing with\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(model_LAS, param_grid=param_grid,n_jobs=-1)\n",
    "grid_search.fit(prostate.features, prostate.target)\n",
    "print(\"Best\", grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the grid search 3 different times, adjusting the alphas back up this time, I believe that using an alpha of 0.1 will be best for my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Lasso Model With Best Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Coef 1.6699333664535905 [ 0.57700424  0.06179715 -0.00577247  0.07308461  0.          0.\n",
      " -0.          0.00677141]\n",
      "MSE 0.5576559643302158\n"
     ]
    }
   ],
   "source": [
    "# Adjust the alpha to 0.1 as indicated from the grid search\n",
    "model_LAS = Lasso(alpha=0.1)\n",
    "model_LAS.fit(prostate.features, prostate.target)\n",
    "print(model_LAS)\n",
    "# make predictions\n",
    "expected_LAS = prostate.target\n",
    "predicted_LAS = model_LAS.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_LAS.intercept_,model_LAS.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_LAS, predicted_LAS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my final Lasso model, I used my best alpha of 0.1 that was produced from my 3 separate grid searches and narrowing down my results.  For this model, that alpha value gives an output of a MSE of 0.5576 which is not our best result from the Lasso model, and not as good as our best results from the Ridge models. My best Lasso model had an alpha of 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet Regression\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet(alpha=1, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Coef 1.9313322879124133 [0.15562453 0.         0.         0.         0.         0.\n",
      " 0.         0.01382035]\n",
      "MSE 0.8798461494497223\n"
     ]
    }
   ],
   "source": [
    "# Basic ElasticNet Regression Model\n",
    "model_EN = ElasticNet(alpha=1)\n",
    "model_EN.fit(prostate.features, prostate.target)\n",
    "print(model_EN)\n",
    "# make predictions\n",
    "expected_EN = prostate.target\n",
    "predicted_EN = model_EN.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_EN.intercept_, model_EN.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_EN, predicted_EN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first ElasticNet model used the default values, which means an alpha of 1.  This produced an MSE of 0.8798."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Coef 1.734484615564524 [0.36370419 0.         0.         0.         0.         0.\n",
      " 0.         0.01037257]\n",
      "MSE 0.6912923472841728\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet Regression Model adjusting Alpha from 1 to 0.5\n",
    "model_EN = ElasticNet(alpha=0.5)\n",
    "model_EN.fit(prostate.features, prostate.target)\n",
    "print(model_EN)\n",
    "# make predictions\n",
    "expected_EN = prostate.target\n",
    "predicted_EN = model_EN.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_EN.intercept_, model_EN.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_EN, predicted_EN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My second ElasticNet model adjusted the alpha down to 0.5 which produced a lower MSE of 0.6912."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet(alpha=0.05, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Coef 1.2225577086019637 [ 0.55774379  0.32375852 -0.01396712  0.09627958  0.38238527 -0.\n",
      "  0.          0.00491596]\n",
      "MSE 0.4755112050977968\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet Regression Model adjusting Alpha from 0.5 to 0.05\n",
    "model_EN = ElasticNet(alpha=0.05)\n",
    "model_EN.fit(prostate.features, prostate.target)\n",
    "print(model_EN)\n",
    "# make predictions\n",
    "expected_EN = prostate.target\n",
    "predicted_EN = model_EN.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_EN.intercept_, model_EN.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_EN, predicted_EN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My 3rd model used an alpha of 0.05, which finally saw my MSE go down to 0.4755.  This did lower the MSE, but I will next use a grid search combined with cross validation to see if continued dropping of the alpha can bring my MSE even lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search to narrow down alpha with Cross Validation 5 Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'alpha': 0.25}\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over several parameters\n",
    "param_grid = {\"alpha\": [0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.25]}  #different penalties I am playing with\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(model_EN, param_grid=param_grid,n_jobs=-1, cv = 5)\n",
    "grid_search.fit(prostate.features, prostate.target)\n",
    "print(\"Best\", grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search between 9 alpha values and a cross validation being performed 5 times yields a best alpha result of 0.25.  My grid search included values below and above the previously used alpha of 0.05.  The cross validation allowed each of those alphas to run through the model 5 times using separate samples of data.  The combination of the alphas in the grid search and the cross validation produced a best alpha of 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet(alpha=0.25, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Coef 1.707335291612384 [ 0.49643943  0.         -0.0014447   0.05931984  0.          0.00820404\n",
      " -0.          0.00773701]\n",
      "MSE 0.5967985948322396\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet Regression Model adjusting Alpha to 0.0001 as indicated as the best alpha for the model\n",
    "model_EN = ElasticNet(alpha=0.25)\n",
    "model_EN.fit(prostate.features, prostate.target)\n",
    "print(model_EN)\n",
    "# make predictions\n",
    "expected_EN = prostate.target\n",
    "predicted_EN = model_EN.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_EN.intercept_, model_EN.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_EN, predicted_EN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My final version of the ElasticNet model used my best alpha from above, 0.25.  This produced a MSE for my ElasticNet Regressions of 0.5967 which is not my best MSE for the ElasticNet telling me there would be additional parameters that need adjusting to continue further lowering the MSE for my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD Regressor\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
      "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "       loss='squared_loss', max_iter=None, n_iter=None, penalty='l2',\n",
      "       power_t=0.25, random_state=None, shuffle=True, tol=None, verbose=0,\n",
      "       warm_start=False)\n",
      "Coef [-6.77929694e+08] [-7.70009280e+09  4.58271754e+10  8.92255776e+10 -8.27413691e+10\n",
      " -2.82687433e+10 -2.41121970e+10 -3.09273183e+10  1.00083452e+11]\n",
      "MSE 7.421110338012683e+25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM025575\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Basic SGD Regressor Model, l2 penalty \n",
    "model_SGD = SGDRegressor()\n",
    "model_SGD.fit(prostate.features, prostate.target)\n",
    "print(model_SGD)\n",
    "# make predictions\n",
    "expected_SGD = prostate.target\n",
    "predicted_SGD = model_SGD.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_SGD.intercept_, model_SGD.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_SGD, predicted_SGD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first SGD Regressor model using all of the default parameter values, produced an MSE of 7.4211."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor(alpha=0.01, average=False, epsilon=0.1, eta0=0.01,\n",
      "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "       loss='squared_loss', max_iter=None, n_iter=None, penalty='l1',\n",
      "       power_t=0.25, random_state=None, shuffle=True, tol=None, verbose=0,\n",
      "       warm_start=False)\n",
      "Coef [6.30808785e+09] [ 3.38186965e+10 -4.48634201e+09 -2.58242536e+10 -1.36546973e+11\n",
      "  5.39619927e+09 -4.24063811e+10  7.80686344e+10 -1.72911269e+11]\n",
      "MSE 5.224196215581104e+25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM025575\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# SGD Regressor Model, adjusting penalty to l1 and alpha to 0.01\n",
    "model_SGD = SGDRegressor(alpha = 0.01, penalty = 'l1')\n",
    "model_SGD.fit(prostate.features, prostate.target)\n",
    "print(model_SGD)\n",
    "# make predictions\n",
    "expected_SGD = prostate.target\n",
    "predicted_SGD = model_SGD.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_SGD.intercept_, model_SGD.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_SGD, predicted_SGD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my second model, I adjusted my alpha up to 0.01 from the default of 0.0001 and changed my penalty from the default L2 to L1. These changes caused my MSE to go down from the basic model to 5.2241."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor(alpha=1e-05, average=False, epsilon=0.1, eta0=0.01,\n",
      "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "       loss='squared_loss', max_iter=None, n_iter=None, penalty='l2',\n",
      "       power_t=0.25, random_state=None, shuffle=True, tol=None, verbose=0,\n",
      "       warm_start=False)\n",
      "Coef [-2.25924964e+09] [ 7.45251104e+10 -1.98566627e+10  9.36716234e+10 -1.22394548e+11\n",
      " -7.50006899e+09 -2.99797502e+10 -5.90037277e+08 -6.86371648e+10]\n",
      "MSE 2.21168628916299e+25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM025575\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# SGD Regressor Model, adjusting penalty to l1 and alpha to 0.00001\n",
    "model_SGD = SGDRegressor(alpha = 0.00001, penalty = 'l2')\n",
    "model_SGD.fit(prostate.features, prostate.target)\n",
    "print(model_SGD)\n",
    "# make predictions\n",
    "expected_SGD = prostate.target\n",
    "predicted_SGD = model_SGD.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_SGD.intercept_, model_SGD.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_SGD, predicted_SGD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My third model I set my penalty back to at L2, and adjusted the alpha below the default value to 0.00001.  This lowered my MSE to 2.2116, which was my best SGD model yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search to narrow down alpha with Cross Validation 10 Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'penalty': 'none'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM025575\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over several parameters\n",
    "param_grid = {\"penalty\": ['none', 'l2', 'l1', 'elasticnet']}  #different penalties I am playing with\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(model_SGD, param_grid=param_grid,n_jobs=-1, cv = 10)\n",
    "grid_search.fit(prostate.features, prostate.target)\n",
    "print(\"Best\", grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I used a grid search using the 4 possible penalty values and a cross validation ten times to see which of the penalty values is best to use.  My result was that the best penalty to use is 'none'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor(alpha=1e-05, average=False, epsilon=0.1, eta0=0.01,\n",
      "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "       loss='squared_loss', max_iter=None, n_iter=None, penalty='none',\n",
      "       power_t=0.25, random_state=None, shuffle=True, tol=None, verbose=0,\n",
      "       warm_start=False)\n",
      "Coef [-9.04905494e+08] [-2.38270854e+09  2.25756649e+09 -6.29617672e+10 -2.07780326e+10\n",
      "  2.20871857e+10  1.21835735e+11  2.52941673e+09 -2.21714819e+11]\n",
      "MSE 1.2800948445792925e+26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM025575\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# SGD Regressor Model, adjusting penalty 'none' based on the grid search results and cross validation\n",
    "model_SGD = SGDRegressor(alpha = 0.00001, penalty = 'none')\n",
    "model_SGD.fit(prostate.features, prostate.target)\n",
    "print(model_SGD)\n",
    "# make predictions\n",
    "expected_SGD = prostate.target\n",
    "predicted_SGD = model_SGD.predict(prostate.features)\n",
    "# summarize the fit of the model\n",
    "print(\"Coef\", model_SGD.intercept_, model_SGD.coef_)\n",
    "print(\"MSE\", mean_squared_error(expected_SGD, predicted_SGD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I used the result from my grid search and cross validation and paired that with the lowest alpha I had used and ended up getting my lowest MSE yet with this model, 1.28009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hold for explanation of what best model is and why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing my 4 different Ridge, Lasso, ElasticNet, and SGD Regressor, the best model I would say that my best model was my third Ridge Regression model, where I used an alpha of 0.5 and got a MSE of 0.4554.  I got various other MSE values that came close to that after tuning those models in various ways and using grid search and cross validation to more efficiently search for my best alpha and penalty values.  However, based solely on the lowest MSE, my Ridge Regression Model with the alpha of 0.5 is my best performing model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
