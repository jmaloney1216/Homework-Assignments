{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages.\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline \n",
    "np.set_printoptions(suppress=True)\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Scikit Learn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection  import train_test_split, cross_val_score, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\JM025575\\\\Predictive Models Class'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM025575\\Predictive Models Class\\data\n"
     ]
    }
   ],
   "source": [
    "cd /Users/JM025575/Predictive Models Class/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment I will be using the adult census dataset.  I downloaded the data locally and will now load it in to my notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week         country  salary  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Using Pandas\n",
    "adult = pd.read_csv(\"adult.csv\") #,thousands=','\n",
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Validating that there are no null values\n",
    "feat_miss = adult.columns[adult.isnull().any()]\n",
    "print(feat_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'workclass',\n",
       " 'fnlwgt',\n",
       " 'education',\n",
       " 'education-num',\n",
       " 'marital-status',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'capital-gain',\n",
       " 'capital-loss',\n",
       " 'hours-per-week',\n",
       " 'country',\n",
       " 'salary']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Review Column Names\n",
    "adult.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be setting salary as the target variable.  We will do some exploratory analysis on the variables and get the target feature to the first column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Target Variable and Move to Target to Column 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   salary  age          workclass  fnlwgt   education  education-num  \\\n",
       "0   <=50K   39          State-gov   77516   Bachelors             13   \n",
       "1   <=50K   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   <=50K   38            Private  215646     HS-grad              9   \n",
       "3   <=50K   53            Private  234721        11th              7   \n",
       "4   <=50K   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week         country  \n",
       "0          2174             0              40   United-States  \n",
       "1             0             0              13   United-States  \n",
       "2             0             0              40   United-States  \n",
       "3             0             0              40   United-States  \n",
       "4             0             0              40            Cuba  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# designate target variable name and move the target variable of 'churn' to my first column for easier use.\n",
    "targetName = 'salary'\n",
    "targetSeries = adult[targetName]\n",
    "#remove target from current location and insert in collum 0\n",
    "del adult[targetName]\n",
    "adult.insert(0, targetName, targetSeries)\n",
    "#reprint dataframe and see target is in position 0\n",
    "adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns I do not believe will contribute to a strong prediction\n",
    "adult = adult.drop(['country'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to drop the 'country' column as the large majority of results are either United States or have no information.  Since I intend to transform my categorical variables to dummies, and with the weighting so heavy towards the US, I did not want to create a data set with such a large number of features where I do not believe the country features that would be created would have a strong predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['salary', 'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
       "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
       "       'capital-gain', 'capital-loss', 'hours-per-week'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show columns\n",
    "adult.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 14)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salary            object\n",
       "age                int64\n",
       "workclass         object\n",
       "fnlwgt             int64\n",
       "education         object\n",
       "education-num      int64\n",
       "marital-status    object\n",
       "occupation        object\n",
       "relationship      object\n",
       "race              object\n",
       "sex               object\n",
       "capital-gain       int64\n",
       "capital-loss       int64\n",
       "hours-per-week     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show data types of all features\n",
    "adult.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                   38.581647\n",
       "fnlwgt            189778.366512\n",
       "education-num         10.080679\n",
       "capital-gain        1077.648844\n",
       "capital-loss          87.303830\n",
       "hours-per-week        40.437456\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean of all numerical attributes\n",
    "adult.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                   13.640433\n",
       "fnlwgt            105549.977697\n",
       "education-num          2.572720\n",
       "capital-gain        7385.292085\n",
       "capital-loss         402.960219\n",
       "hours-per-week        12.347429\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#standard deviation of all numerical attributes\n",
    "adult.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salary\n",
       " <=50K    24720\n",
       " >50K      7841\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Salary Distribution\n",
    "adult.groupby('salary').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "education\n",
       " 10th              933\n",
       " 11th             1175\n",
       " 12th              433\n",
       " 1st-4th           168\n",
       " 5th-6th           333\n",
       " 7th-8th           646\n",
       " 9th               514\n",
       " Assoc-acdm       1067\n",
       " Assoc-voc        1382\n",
       " Bachelors        5355\n",
       " Doctorate         413\n",
       " HS-grad         10501\n",
       " Masters          1723\n",
       " Preschool          51\n",
       " Prof-school       576\n",
       " Some-college     7291\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Education Distribution\n",
    "adult.groupby('education').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workclass\n",
       " ?                    1836\n",
       " Federal-gov           960\n",
       " Local-gov            2093\n",
       " Never-worked            7\n",
       " Private             22696\n",
       " Self-emp-inc         1116\n",
       " Self-emp-not-inc     2541\n",
       " State-gov            1298\n",
       " Without-pay            14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Workclass Distribution\n",
    "adult.groupby('workclass').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marital-status\n",
       " Divorced                  4443\n",
       " Married-AF-spouse           23\n",
       " Married-civ-spouse       14976\n",
       " Married-spouse-absent      418\n",
       " Never-married            10683\n",
       " Separated                 1025\n",
       " Widowed                    993\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Marital Status Distribution\n",
    "adult.groupby('marital-status').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "occupation\n",
       " ?                    1843\n",
       " Adm-clerical         3770\n",
       " Armed-Forces            9\n",
       " Craft-repair         4099\n",
       " Exec-managerial      4066\n",
       " Farming-fishing       994\n",
       " Handlers-cleaners    1370\n",
       " Machine-op-inspct    2002\n",
       " Other-service        3295\n",
       " Priv-house-serv       149\n",
       " Prof-specialty       4140\n",
       " Protective-serv       649\n",
       " Sales                3650\n",
       " Tech-support          928\n",
       " Transport-moving     1597\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Occupation Distribution\n",
    "adult.groupby('occupation').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "race\n",
       " Amer-Indian-Eskimo      311\n",
       " Asian-Pac-Islander     1039\n",
       " Black                  3124\n",
       " Other                   271\n",
       " White                 27816\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Race Distribution\n",
    "adult.groupby('race').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex\n",
       " Female    10771\n",
       " Male      21790\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gender Distribution\n",
    "adult.groupby('sex').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z-Score Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through and run a z-score normalization for all numeric values to ensure no one feature has an undue influence on our models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform numeric features to Z-Scores\n",
    "adult.age=pd.DataFrame((adult.age - adult.age.mean())/adult.age.std())\n",
    "adult.fnlwgt=pd.DataFrame((adult.fnlwgt - adult.fnlwgt.mean())/adult.fnlwgt.std())\n",
    "adult['education-num']=pd.DataFrame((adult['education-num'] - adult['education-num'].mean())/adult['education-num'].std())\n",
    "adult['capital-gain']=pd.DataFrame((adult['capital-gain'] - adult['capital-gain'].mean())/adult['capital-gain'].std())\n",
    "adult['capital-loss']=pd.DataFrame((adult['capital-loss'] - adult['capital-loss'].mean())/adult['capital-loss'].std())\n",
    "adult['hours-per-week']=pd.DataFrame((adult['hours-per-week'] - adult['hours-per-week'].mean())/adult['hours-per-week'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0.030670</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>-1.063594</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>1.134721</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.148451</td>\n",
       "      <td>-0.216656</td>\n",
       "      <td>-0.035429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0.837096</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>-1.008692</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>1.134721</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>-0.145918</td>\n",
       "      <td>-0.216656</td>\n",
       "      <td>-2.222119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>-0.042641</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.245075</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>-0.420053</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>-0.145918</td>\n",
       "      <td>-0.216656</td>\n",
       "      <td>-0.035429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>1.057031</td>\n",
       "      <td>Private</td>\n",
       "      <td>0.425795</td>\n",
       "      <td>11th</td>\n",
       "      <td>-1.197440</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>-0.145918</td>\n",
       "      <td>-0.216656</td>\n",
       "      <td>-0.035429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>-0.775756</td>\n",
       "      <td>Private</td>\n",
       "      <td>1.408154</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>1.134721</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>-0.145918</td>\n",
       "      <td>-0.216656</td>\n",
       "      <td>-0.035429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   salary       age          workclass    fnlwgt   education  education-num  \\\n",
       "0   <=50K  0.030670          State-gov -1.063594   Bachelors       1.134721   \n",
       "1   <=50K  0.837096   Self-emp-not-inc -1.008692   Bachelors       1.134721   \n",
       "2   <=50K -0.042641            Private  0.245075     HS-grad      -0.420053   \n",
       "3   <=50K  1.057031            Private  0.425795        11th      -1.197440   \n",
       "4   <=50K -0.775756            Private  1.408154   Bachelors       1.134721   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  \n",
       "0      0.148451     -0.216656       -0.035429  \n",
       "1     -0.145918     -0.216656       -2.222119  \n",
       "2     -0.145918     -0.216656       -0.035429  \n",
       "3     -0.145918     -0.216656       -0.035429  \n",
       "4     -0.145918     -0.216656       -0.035429  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables to Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le_dep = preprocessing.LabelEncoder()\n",
    "#to convert into numbers\n",
    "adult['salary'] = le_dep.fit_transform(adult['salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform data transformation. Creates dummies of any categorical feature and turns them in to their own column with values of \n",
    "# 0 or 1.\n",
    "for col in adult.columns[1:]:\n",
    "\tattName = col\n",
    "\tdType = adult[col].dtype\n",
    "\tmissing = pd.isnull(adult[col]).any()\n",
    "\tuniqueCount = len(adult[attName].value_counts(normalize=False))\n",
    "\t# discretize (create dummies)\n",
    "\tif dType == object:\n",
    "\t\tadult = pd.concat([adult, pd.get_dummies(adult[col], prefix=col)], axis=1)\n",
    "\t\tdel adult[attName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 67)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I went from 15 features to 67 features now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_ ?</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>...</th>\n",
       "      <th>relationship_ Own-child</th>\n",
       "      <th>relationship_ Unmarried</th>\n",
       "      <th>relationship_ Wife</th>\n",
       "      <th>race_ Amer-Indian-Eskimo</th>\n",
       "      <th>race_ Asian-Pac-Islander</th>\n",
       "      <th>race_ Black</th>\n",
       "      <th>race_ Other</th>\n",
       "      <th>race_ White</th>\n",
       "      <th>sex_ Female</th>\n",
       "      <th>sex_ Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.030670</td>\n",
       "      <td>-1.063594</td>\n",
       "      <td>1.134721</td>\n",
       "      <td>0.148451</td>\n",
       "      <td>-0.216656</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.837096</td>\n",
       "      <td>-1.008692</td>\n",
       "      <td>1.134721</td>\n",
       "      <td>-0.145918</td>\n",
       "      <td>-0.216656</td>\n",
       "      <td>-2.222119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.042641</td>\n",
       "      <td>0.245075</td>\n",
       "      <td>-0.420053</td>\n",
       "      <td>-0.145918</td>\n",
       "      <td>-0.216656</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.057031</td>\n",
       "      <td>0.425795</td>\n",
       "      <td>-1.197440</td>\n",
       "      <td>-0.145918</td>\n",
       "      <td>-0.216656</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.775756</td>\n",
       "      <td>1.408154</td>\n",
       "      <td>1.134721</td>\n",
       "      <td>-0.145918</td>\n",
       "      <td>-0.216656</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   salary       age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "0       0  0.030670 -1.063594       1.134721      0.148451     -0.216656   \n",
       "1       0  0.837096 -1.008692       1.134721     -0.145918     -0.216656   \n",
       "2       0 -0.042641  0.245075      -0.420053     -0.145918     -0.216656   \n",
       "3       0  1.057031  0.425795      -1.197440     -0.145918     -0.216656   \n",
       "4       0 -0.775756  1.408154       1.134721     -0.145918     -0.216656   \n",
       "\n",
       "   hours-per-week  workclass_ ?  workclass_ Federal-gov  workclass_ Local-gov  \\\n",
       "0       -0.035429             0                       0                     0   \n",
       "1       -2.222119             0                       0                     0   \n",
       "2       -0.035429             0                       0                     0   \n",
       "3       -0.035429             0                       0                     0   \n",
       "4       -0.035429             0                       0                     0   \n",
       "\n",
       "     ...      relationship_ Own-child  relationship_ Unmarried  \\\n",
       "0    ...                            0                        0   \n",
       "1    ...                            0                        0   \n",
       "2    ...                            0                        0   \n",
       "3    ...                            0                        0   \n",
       "4    ...                            0                        0   \n",
       "\n",
       "   relationship_ Wife  race_ Amer-Indian-Eskimo  race_ Asian-Pac-Islander  \\\n",
       "0                   0                         0                         0   \n",
       "1                   0                         0                         0   \n",
       "2                   0                         0                         0   \n",
       "3                   0                         0                         0   \n",
       "4                   1                         0                         0   \n",
       "\n",
       "   race_ Black  race_ Other  race_ White  sex_ Female  sex_ Male  \n",
       "0            0            0            1            0          1  \n",
       "1            0            0            1            0          1  \n",
       "2            0            0            1            0          1  \n",
       "3            1            0            0            0          1  \n",
       "4            1            0            0            1          0  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset into Train/Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will create a training and test data set that will be used in the models below.  I will use a 33/67 split for the test vs train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into testing and training, creating 4 new objects here. Separating out features from target.  \n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    adult.iloc[:,1:].values, adult.iloc[:,0].values, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four new train/test files and their shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10746, 66)\n",
      "(21815, 66)\n",
      "(10746,)\n",
      "(21815,)\n"
     ]
    }
   ],
   "source": [
    "print(features_test.shape)\n",
    "print(features_train.shape)\n",
    "print(target_test.shape)\n",
    "print(target_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first run KNN classification models.  I will run a minimum of 3 models for this section, each with different tuning parameters to try and get a higher accuracy score each time.  The first model will only have the n_neighbors parameter changed to 3.  I will continue to adjust my remaining models from there and potentially run a grid search to find my ideal number of neighbors with the goal of maximizing the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "[0 0 0 ... 0 1 0]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.90      0.89      8151\n",
      "          1       0.65      0.60      0.62      2595\n",
      "\n",
      "avg / total       0.82      0.83      0.82     10746\n",
      "\n",
      "[[7314  837]\n",
      " [1043 1552]]\n",
      "0.8250511818351014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(features_train, target_train) \n",
    "predicted_KNN1 = neigh.predict(features_test)\n",
    "print(neigh)\n",
    "# make predictions\n",
    "print(target_test)\n",
    "# summarize the fit of the model\n",
    "print(classification_report(target_test, predicted_KNN1))\n",
    "print(confusion_matrix(target_test, predicted_KNN1))\n",
    "print(accuracy_score(target_test,predicted_KNN1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.83684693 0.8299725  0.82676444 0.82951421 0.82401467 0.82768103\n",
      " 0.80513526 0.82714351 0.81843191 0.82522936]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.825073380815709"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify KNN with Cross Validation\n",
    "scores_KNN = cross_val_score(neigh, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_KNN)\n",
    "scores_KNN.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=7, p=1,\n",
      "           weights='uniform')\n",
      "[0 0 0 ... 0 1 0]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.91      0.89      8151\n",
      "          1       0.68      0.59      0.63      2595\n",
      "\n",
      "avg / total       0.83      0.83      0.83     10746\n",
      "\n",
      "[[7431  720]\n",
      " [1071 1524]]\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=7, p = 1)\n",
    "neigh.fit(features_train, target_train) \n",
    "predicted_KNN1 = neigh.predict(features_test)\n",
    "print(neigh)\n",
    "# make predictions\n",
    "print(target_test)\n",
    "# summarize the fit of the model\n",
    "print(classification_report(target_test, predicted_KNN1))\n",
    "print(confusion_matrix(target_test, predicted_KNN1))\n",
    "print(accuracy_score(target_test,predicted_KNN1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.84647113 0.84097159 0.83088909 0.83363886 0.83455545 0.82630614\n",
      " 0.82255846 0.82897753 0.8262265  0.84678899]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8337383749072375"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify KNN with Cross Validation\n",
    "scores_KNN = cross_val_score(neigh, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_KNN)\n",
    "scores_KNN.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=15, p=1,\n",
      "           weights='distance')\n",
      "[0 0 0 ... 0 1 0]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.92      0.89      8151\n",
      "          1       0.69      0.59      0.64      2595\n",
      "\n",
      "avg / total       0.83      0.84      0.83     10746\n",
      "\n",
      "[[7459  692]\n",
      " [1059 1536]]\n",
      "0.8370556486134375\n"
     ]
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors = 15, p = 1, weights = 'distance')\n",
    "neigh.fit(features_train, target_train) \n",
    "predicted_KNN1 = neigh.predict(features_test)\n",
    "print(neigh)\n",
    "# make predictions\n",
    "print(target_test)\n",
    "# summarize the fit of the model\n",
    "print(classification_report(target_test, predicted_KNN1))\n",
    "print(confusion_matrix(target_test, predicted_KNN1))\n",
    "print(accuracy_score(target_test,predicted_KNN1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.85472044 0.84280477 0.83455545 0.84372136 0.8395967  0.82951421\n",
      " 0.83402109 0.82760202 0.83402109 0.84220183]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.838275895869369"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify KNN with Cross Validation\n",
    "scores_KNN = cross_val_score(neigh, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_KNN)\n",
    "scores_KNN.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search to narrow down n_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'n_neighbors': 16}\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over several parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"n_neighbors\": [12, 16, 20]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(neigh, param_grid=param_grid,n_jobs=-1)\n",
    "grid_search.fit(features_train, target_train)\n",
    "print(\"Best\", grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=16, p=1,\n",
      "           weights='distance')\n",
      "[0 0 0 ... 0 1 0]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.91      0.89      8151\n",
      "          1       0.69      0.59      0.64      2595\n",
      "\n",
      "avg / total       0.83      0.84      0.83     10746\n",
      "\n",
      "[[7452  699]\n",
      " [1052 1543]]\n",
      "0.8370556486134375\n"
     ]
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors = 16, p = 1, weights = 'distance')\n",
    "neigh.fit(features_train, target_train) \n",
    "predicted_KNN1 = neigh.predict(features_test)\n",
    "print(neigh)\n",
    "# make predictions\n",
    "print(target_test)\n",
    "# summarize the fit of the model\n",
    "print(classification_report(target_test, predicted_KNN1))\n",
    "print(confusion_matrix(target_test, predicted_KNN1))\n",
    "print(accuracy_score(target_test,predicted_KNN1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.85059578 0.84326306 0.83638863 0.84509624 0.83501375 0.82768103\n",
      " 0.83264558 0.83035305 0.83539661 0.84357798]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8380011709988473"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify KNN with Cross Validation\n",
    "scores_KNN = cross_val_score(neigh, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_KNN)\n",
    "scores_KNN.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran 4 separate KNN models, all with slightly different parameters tuned.  The first model I only changed the n_neighbors to 3 and got an accuracy score of 0.825.  I cross validated this model 10 times to check for overfitting and stayed within 1% on all models.  The second model I ran I adjusted the n_neighbors to 7 and changed to p=1 to use the manhattan distance versus the euclidean distance.  This time my accuracy score increased to 0.833.  I also cross validated 10 times and saw no overfitting.  My third model I adjusted n_neighbors to 15, kept p=1, and now adjusted my weights= to 'distance' rather than uniform.  This resulted in a slightly better model with an accuracy score of 0.837.  Cross validating again showed a lack of overfitting.  Finally, I ran a grid search to find my best n_neighbors number, which turned out to be 16.  I ran one final model, similar to model 3 but with n_neighbors at 16, and got an accuracy score of 0.837. My final two KNN Models are my best models for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will next run Decision Tree classification models.  I will run a minimum of 3 models for this section, each with different tuning parameters to try and get a higher accuracy score each time.  The first model will be a basic Decision Tree with no default parameters tuned.  I will continue to adjust my remaining models from there with the goal of maximizing the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree - Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree \n",
    "clf_dt = tree.DecisionTreeClassifier() #taking out of box options. \n",
    "#Call up the model to see the parameters you can tune (and their default setting)\n",
    "print(clf_dt)\n",
    "#Fit clf to the training data\n",
    "clf_dt = clf_dt.fit(features_train, target_train)\n",
    "#Predict clf DT model again test data\n",
    "target_predicted_dt = clf_dt.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Accuracy Score 0.8123022520007445\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.88      0.88      8151\n",
      "          1       0.61      0.61      0.61      2595\n",
      "\n",
      "avg / total       0.81      0.81      0.81     10746\n",
      "\n",
      "[[7136 1015]\n",
      " [1002 1593]]\n"
     ]
    }
   ],
   "source": [
    "print(\"DT Accuracy Score\", accuracy_score(target_test, target_predicted_dt))\n",
    "print(classification_report(target_test, target_predicted_dt))\n",
    "print(confusion_matrix(target_test, target_predicted_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validate Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.80843263 0.81897342 0.8203483  0.81576535 0.8111824  0.80522456\n",
      " 0.80788629 0.82393398 0.80284273 0.81284404]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8127433708099001"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify DT with Cross Validation\n",
    "scores = cross_val_score(clf_dt, features_train, target_train, cv=10)  #it is sampling from each of the data sets, usually sampling 50% out of each\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree - Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='random')\n"
     ]
    }
   ],
   "source": [
    "clf_dt = tree.DecisionTreeClassifier(criterion = 'entropy', splitter = 'random' ) #changing the criterion to entropy and splitter to random\n",
    "print(clf_dt)\n",
    "#Fit clf to the training data\n",
    "clf_dt = clf_dt.fit(features_train, target_train)\n",
    "#Predict clf DT model again test data\n",
    "target_predicted_dt = clf_dt.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Accuracy Score 0.8121161362367393\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.87      0.88      8151\n",
      "          1       0.61      0.62      0.61      2595\n",
      "\n",
      "avg / total       0.81      0.81      0.81     10746\n",
      "\n",
      "[[7125 1026]\n",
      " [ 993 1602]]\n"
     ]
    }
   ],
   "source": [
    "print(\"DT Accuracy Score\", accuracy_score(target_test, target_predicted_dt))\n",
    "print(classification_report(target_test, target_predicted_dt))\n",
    "print(confusion_matrix(target_test, target_predicted_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validate Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.82722273 0.80797434 0.80018332 0.81851512 0.80018332 0.81576535\n",
      " 0.80375974 0.80330124 0.80055021 0.80458716]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8082042523143654"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify DT with Cross Validation\n",
    "scores = cross_val_score(clf_dt, features_train, target_train, cv=10)  #it is sampling from each of the data sets, usually sampling 50% out of each\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree - Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=5,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "clf_dt = tree.DecisionTreeClassifier(criterion = 'gini', min_samples_split = 5)\n",
    "print(clf_dt)\n",
    "#Fit clf to the training data\n",
    "clf_dt = clf_dt.fit(features_train, target_train)\n",
    "#Predict clf DT model again test data\n",
    "target_predicted_dt = clf_dt.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Accuracy Score 0.8221663874930206\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.89      0.88      8151\n",
      "          1       0.64      0.61      0.62      2595\n",
      "\n",
      "avg / total       0.82      0.82      0.82     10746\n",
      "\n",
      "[[7262  889]\n",
      " [1022 1573]]\n"
     ]
    }
   ],
   "source": [
    "print(\"DT Accuracy Score\", accuracy_score(target_test, target_predicted_dt))\n",
    "print(classification_report(target_test, target_predicted_dt))\n",
    "print(confusion_matrix(target_test, target_predicted_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validate Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.81759853 0.82309808 0.82676444 0.82172319 0.81714024 0.8111824\n",
      " 0.82530949 0.82072444 0.81201284 0.82247706]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8198030706185142"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify DT with Cross Validation\n",
    "scores = cross_val_score(clf_dt, features_train, target_train, cv=10)  #it is sampling from each of the data sets, usually sampling 50% out of each\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran 3 separate Decision Tree models, all with slightly different parameters tuned.  The first model had no default parameters tuned and got an accuracy score of 0.812.  I cross validated this model 10 times to check for overfitting and stayed within 2% on all models.  The second model I ran I adjusted the criterion to entropy, for information gain, and the splitter to random.  This time my accuracy score slightly increased to 0.812.  I also cross validated 10 times and saw no overfitting.  My final model I adjusted criterion back to gini and set my minimum samples split to 5 for the number of samples required for a split.  This resulted in a slightly better model with an accuracy score of 0.822.  Cross validating again showed a lack of overfitting.  My final Decision Tree is my best model for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will next run Stochastic Gradient Descent Models.  I will run a minimum of 3 models for this section, each with different tuning parameters to try and get a higher accuracy score each time.  The first model will be a SGD model with no parameters set, so a basic model.  I will continue to adjust my remaining models from there and may run a grid search to find my ideal alpha or other parameters with the goal of maximizing the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_sgd = SGDClassifier()\n",
    "print(clf_sgd)\n",
    "#Fit clf_sgd to the training data\n",
    "clf_sgd = clf_sgd.fit(features_train, target_train)\n",
    "#Predict clf_sgd model again test data\n",
    "predicted_sgd = clf_sgd.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Accuracy Score 0.8455239158756747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.89      0.88      8151\n",
      "          1       0.64      0.61      0.62      2595\n",
      "\n",
      "avg / total       0.82      0.82      0.82     10746\n",
      "\n",
      "[[7625  526]\n",
      " [1134 1461]]\n"
     ]
    }
   ],
   "source": [
    "print(\"SGD Accuracy Score\", accuracy_score(target_test, predicted_sgd))\n",
    "print(classification_report(target_test, target_predicted_dt))\n",
    "print(confusion_matrix(target_test, predicted_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validate Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.81393217 0.83730522 0.81530706 0.80889093 0.80797434 0.82676444\n",
      " 0.83310408 0.82026593 0.81338835 0.83027523]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8207207749233014"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify SGD with Cross Validation\n",
    "scores_sgd = cross_val_score(clf_sgd, features_train, target_train, cv=10)  #it is sampling from each of the data sets, usually sampling 50% out of each\n",
    "print(\"Cross Validation Score for each K\",scores_sgd)\n",
    "scores_sgd.mean()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.1, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='l1', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_sgd = SGDClassifier(penalty = 'l1', alpha = 0.1, )\n",
    "print(clf_sgd)\n",
    "#Fit clf_sgd to the training data\n",
    "clf_sgd = clf_sgd.fit(features_train, target_train)\n",
    "#Predict clf_sgd model again test data\n",
    "predicted_sgd = clf_sgd.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Accuracy Score 0.7631676903033687\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.89      0.88      8151\n",
      "          1       0.64      0.61      0.62      2595\n",
      "\n",
      "avg / total       0.82      0.82      0.82     10746\n",
      "\n",
      "[[8151    0]\n",
      " [2545   50]]\n"
     ]
    }
   ],
   "source": [
    "print(\"SGD Accuracy Score\", accuracy_score(target_test, predicted_sgd))\n",
    "print(classification_report(target_test, target_predicted_dt))\n",
    "print(confusion_matrix(target_test, predicted_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.76260312 0.76581118 0.75939505 0.76535289 0.76351971 0.763978\n",
      " 0.7647868  0.76249427 0.75974324 0.75963303]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7627317273306977"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify SGD with Cross Validation\n",
    "scores_sgd = cross_val_score(clf_sgd, features_train, target_train, cv=10)  #it is sampling from each of the data sets, usually sampling 50% out of each\n",
    "print(\"Cross Validation Score for each K\",scores_sgd)\n",
    "scores_sgd.mean()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.01, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_sgd = SGDClassifier(penalty = 'l2', alpha = 0.01, )\n",
    "print(clf_sgd)\n",
    "#Fit clf_sgd to the training data\n",
    "clf_sgd = clf_sgd.fit(features_train, target_train)\n",
    "#Predict clf_sgd model again test data\n",
    "predicted_sgd = clf_sgd.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Accuracy Score 0.847571189279732\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.89      0.88      8151\n",
      "          1       0.64      0.61      0.62      2595\n",
      "\n",
      "avg / total       0.82      0.82      0.82     10746\n",
      "\n",
      "[[7663  488]\n",
      " [1150 1445]]\n"
     ]
    }
   ],
   "source": [
    "print(\"SGD Accuracy Score\", accuracy_score(target_test, predicted_sgd))\n",
    "print(classification_report(target_test, target_predicted_dt))\n",
    "print(confusion_matrix(target_test, predicted_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.8492209  0.85242896 0.84509624 0.85334555 0.84280477 0.84326306\n",
      " 0.84640073 0.84594223 0.83768913 0.84311927]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8459310848133583"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify SGD with Cross Validation\n",
    "scores_sgd = cross_val_score(clf_sgd, features_train, target_train, cv=10)  #it is sampling from each of the data sets, usually sampling 50% out of each\n",
    "print(\"Cross Validation Score for each K\",scores_sgd)\n",
    "scores_sgd.mean()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search to narrow down alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"alpha\": [0.01, 0.1, 0.001, 0.00001, 1]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf_sgd, param_grid=param_grid,n_jobs=-1)\n",
    "grid_search.fit(features_train, target_train)\n",
    "print(\"Best\", grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Model - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_sgd = SGDClassifier(penalty = 'l2', alpha = 0.001, )\n",
    "print(clf_sgd)\n",
    "#Fit clf_sgd to the training data\n",
    "clf_sgd = clf_sgd.fit(features_train, target_train)\n",
    "#Predict clf_sgd model again test data\n",
    "predicted_sgd = clf_sgd.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Accuracy Score 0.8456169737576773\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.89      0.88      8151\n",
      "          1       0.64      0.61      0.62      2595\n",
      "\n",
      "avg / total       0.82      0.82      0.82     10746\n",
      "\n",
      "[[7376  775]\n",
      " [ 884 1711]]\n"
     ]
    }
   ],
   "source": [
    "print(\"SGD Accuracy Score\", accuracy_score(target_test, predicted_sgd))\n",
    "print(classification_report(target_test, target_predicted_dt))\n",
    "print(confusion_matrix(target_test, predicted_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.85334555 0.85747021 0.84509624 0.85747021 0.84051329 0.84463795\n",
      " 0.84548372 0.84869326 0.84181568 0.85321101]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8487737128635316"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify SGD with Cross Validation\n",
    "scores_sgd = cross_val_score(clf_sgd, features_train, target_train, cv=10)  #it is sampling from each of the data sets, usually sampling 50% out of each\n",
    "print(\"Cross Validation Score for each K\",scores_sgd)\n",
    "scores_sgd.mean()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran 4 separate Stochastic Gradient Descent models, all with slightly different parameters tuned.  The first model I ran had no parameters adjusted from the based model and got an accuracy score of 0.825.  I cross validated this model 10 times to check for overfitting and stayed within 2% on all models.  The second model I ran I adjusted my penalty to l1 and the alpha I raised to 0.1.  This time my accuracy score decreased to 0.763.  I also cross validated 10 times and saw no overfitting.  My third model I adjusted my penalty back to l2 and my alpha down to 0.01.  This resulted in a better model with an accuracy score of 0.848.  Cross validating again showed a lack of overfitting.  Finally, I ran a grid search to find my best alpha, which turned out to be 0.001.  I ran one final model, similar to model 3 but with an alpha of 0.001 and a penalty of l2 and got an accuracy score of 0.851. My fourth SGD model produced the best accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will next run Adaboost models.  I will run a minimum of 3 models for this section, each with different tuning parameters to try and get a higher accuracy score each time.  The first model will be an Adaboost model with a Decision Tree with a max depth of 3, and then I will use the SAMME algorithm and n_estimators of 50 for that classifier.  I will continue to adjust my remaining models from there and may run a grid search to find my ideal number of estimators with the goal of maximizing the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Accuracy 0.8536199516099013\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.89      0.92      0.91      8151\n",
      "        Yes       0.72      0.64      0.68      2595\n",
      "\n",
      "avg / total       0.85      0.85      0.85     10746\n",
      "\n",
      "[[7522  629]\n",
      " [ 944 1651]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=50)\n",
    "bdt.fit(features_train, target_train)\n",
    "predicted_bdt=bdt.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Adaboost Accuracy\", accuracy_score(expected,predicted_bdt))\n",
    "print(classification_report(expected, predicted_bdt,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_bdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Adaboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.86480293 0.85609533 0.86021998 0.86388634 0.85013749 0.8492209\n",
      " 0.86795048 0.85878038 0.84915177 0.85779817]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8578043757540297"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Adaboost with Cross Validation\n",
    "scores_ADA = cross_val_score(bdt, features_train, target_train, cv=10) \n",
    "print(\"Cross Validation Score for each K\",scores_ADA)\n",
    "scores_ADA.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Accuracy 0.8553880513679508\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.88      0.94      0.91      8151\n",
      "        Yes       0.76      0.58      0.66      2595\n",
      "\n",
      "avg / total       0.85      0.86      0.85     10746\n",
      "\n",
      "[[7680  471]\n",
      " [1083 1512]]\n"
     ]
    }
   ],
   "source": [
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(criterion = 'entropy', max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=100)\n",
    "bdt.fit(features_train, target_train)\n",
    "predicted_bdt=bdt.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Adaboost Accuracy\", accuracy_score(expected,predicted_bdt))\n",
    "print(classification_report(expected, predicted_bdt,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_bdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Adaboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.86251146 0.8583868  0.86434464 0.86296975 0.84830431 0.84692942\n",
      " 0.85465383 0.85602934 0.8436497  0.85183486]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.854961411668176"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Adaboost with Cross Validation\n",
    "scores_ADA = cross_val_score(bdt, features_train, target_train, cv=10) \n",
    "print(\"Cross Validation Score for each K\",scores_ADA)\n",
    "scores_ADA.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Accuracy 0.8560394565419691\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.88      0.94      0.91      8151\n",
      "        Yes       0.76      0.58      0.66      2595\n",
      "\n",
      "avg / total       0.85      0.86      0.85     10746\n",
      "\n",
      "[[7683  468]\n",
      " [1079 1516]]\n"
     ]
    }
   ],
   "source": [
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(criterion = 'entropy', max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=400, learning_rate = 0.5)\n",
    "bdt.fit(features_train, target_train)\n",
    "predicted_bdt=bdt.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Adaboost Accuracy\", accuracy_score(expected,predicted_bdt))\n",
    "print(classification_report(expected, predicted_bdt,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_bdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Adaboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.86480293 0.85609533 0.86571952 0.8588451  0.85288726 0.84692942\n",
      " 0.8610729  0.85740486 0.84135718 0.85137615]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8556490644927444"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Adaboost with Cross Validation\n",
    "scores_ADA = cross_val_score(bdt, features_train, target_train, cv=10) \n",
    "print(\"Cross Validation Score for each K\",scores_ADA)\n",
    "scores_ADA.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search to narrow down n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"n_estimators\": [300, 400, 500]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(bdt, param_grid=param_grid,n_jobs=-1)\n",
    "grid_search.fit(features_train, target_train)\n",
    "print(\"Best\", grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Model - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Accuracy 0.8560394565419691\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.88      0.94      0.91      8151\n",
      "        Yes       0.76      0.59      0.66      2595\n",
      "\n",
      "avg / total       0.85      0.86      0.85     10746\n",
      "\n",
      "[[7679  472]\n",
      " [1075 1520]]\n"
     ]
    }
   ],
   "source": [
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(criterion = 'entropy', max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=500, learning_rate = 0.5)\n",
    "bdt.fit(features_train, target_train)\n",
    "predicted_bdt=bdt.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Adaboost Accuracy\", accuracy_score(expected,predicted_bdt))\n",
    "print(classification_report(expected, predicted_bdt,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_bdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Adaboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.86434464 0.85747021 0.86159487 0.8588451  0.85517874 0.84830431\n",
      " 0.85969739 0.85878038 0.84181568 0.85321101]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8559242307727647"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Adaboost with Cross Validation\n",
    "scores_ADA = cross_val_score(bdt, features_train, target_train, cv=10) \n",
    "print(\"Cross Validation Score for each K\",scores_ADA)\n",
    "scores_ADA.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran 4 separate Adaboost models, all with slightly different parameters tuned and all using a DecisionTreeClassifier.  The first model I ran had a max_depth of 3 for the decision tree and then used the algorithm SAMME and n_estimators of 50 for the boosting and got an accuracy score of 0.853.  I cross validated this model 10 times to check for overfitting and stayed within 2% on all models.  The second model I ran I adjusted the criterion on the decision tree to 'entropy' and the max_depth to 1, maintained the same algorithm, and increased my n_estimators to 100.  This time my accuracy score increased to 0.855.  I also cross validated 10 times and saw no overfitting.  My third model I adjusted my n_estimators up to 400 and added a learning_rate of 0.5, different from the default of 1.  This resulted in a slightly better model with an accuracy score of 0.856.  Cross validating again showed a lack of overfitting.  Finally, I ran a grid search to find my best n_estimators number, which turned out to be 500.  I ran one final model, similar to model 3 but with n_estimators at 500, and got an accuracy score of 0.856. My third and fourth Adaboost models produced the same accuracy and precision scores, however Model 4 had a slightly higher recall so Model 4 is my best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will next run Random Forest classification models.  I will run a minimum of 3 models for this section, each with different tuning parameters to try and get a higher accuracy score each time.  The first model will be a basic Random Forest model with no default parameters tuned.  I will continue to adjust my remaining models from there and may run a grid search to find my ideal number of estimators with the goal of maximizing the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8465475525777033\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.88      0.93      0.90      8151\n",
      "        Yes       0.73      0.58      0.65      2595\n",
      "\n",
      "avg / total       0.84      0.85      0.84     10746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "# train random forest model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(features_train, target_train)\n",
    "# test random forest model\n",
    "target_predicted_rf = rf.predict(features_test)\n",
    "print(accuracy_score(target_test, target_predicted_rf))\n",
    "target_names = [\"No\", \"Yes\"]\n",
    "print(classification_report(target_test, target_predicted_rf, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.85197067 0.85013749 0.84784601 0.84188818 0.84280477 0.85197067\n",
      " 0.84915177 0.84273269 0.83356259 0.8440367 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8456101521740209"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Random Forest with Cross Validation\n",
    "scores_rf = cross_val_score(rf, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_rf)\n",
    "scores_rf.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8534338358458962\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.89      0.93      0.91      8151\n",
      "        Yes       0.73      0.62      0.67      2595\n",
      "\n",
      "avg / total       0.85      0.85      0.85     10746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators= 100, criterion = 'entropy')\n",
    "rf.fit(features_train, target_train)\n",
    "# test random forest model\n",
    "target_predicted_rf = rf.predict(features_test)\n",
    "print(accuracy_score(target_test, target_predicted_rf))\n",
    "target_names = [\"No\", \"Yes\"]\n",
    "print(classification_report(target_test, target_predicted_rf, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.86984418 0.85792851 0.85472044 0.85976169 0.84647113 0.85563703\n",
      " 0.85740486 0.8436497  0.8523613  0.85917431]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8556953145961564"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Random Forest with Cross Validation\n",
    "scores_rf = cross_val_score(rf, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_rf)\n",
    "scores_rf.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8561325144239718\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.89      0.93      0.91      8151\n",
      "        Yes       0.74      0.63      0.68      2595\n",
      "\n",
      "avg / total       0.85      0.86      0.85     10746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators= 500, n_jobs=4,oob_score=True, criterion = 'entropy')\n",
    "rf.fit(features_train, target_train)\n",
    "# test random forest model\n",
    "target_predicted_rf = rf.predict(features_test)\n",
    "print(accuracy_score(target_test, target_predicted_rf))\n",
    "target_names = [\"No\", \"Yes\"]\n",
    "print(classification_report(target_test, target_predicted_rf, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.86709441 0.86021998 0.85197067 0.85930339 0.84967919 0.8588451\n",
      " 0.85786337 0.84823475 0.84548372 0.8559633 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.855465788654844"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Random Forest with Cross Validation\n",
    "scores_rf = cross_val_score(rf, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_rf)\n",
    "scores_rf.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search to narrow down n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over several parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"n_estimators\": [450, 500, 550]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(rf, param_grid=param_grid,n_jobs=-1)\n",
    "grid_search.fit(features_train, target_train)\n",
    "print(\"Best\", grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran 3 separate Random Forest models, all with slightly different parameters tuned.  The first model had no default parameters tuned and got an accuracy score of 0.846.  I cross validated this model 10 times to check for overfitting and stayed within 2% on all models.  The second model I ran I adjusted the criterion to entropy, for information gain, and the n_estimators to 100, from the default 10.  This time my accuracy score decreased to 0.853.  I also cross validated 10 times and saw no overfitting.  My final model I adjusted criterion to entropy, n_estimators = 500, n_jobs = 4, and my oob_score to True, which let us use use out-of-bag samples. This resulted in a slightly better model with an accuracy score of 0.856.  Cross validating again showed a lack of overfitting.  I ran a grid search as well to try and find my best number of estimators which resulted in my best number still being 500 like I used in my final model. My final Random Forest is my best model for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will next run Bagging Classifier classification models.  I will run a minimum of 3 models for this section, each with different tuning parameters to try and get a higher accuracy score each time.  The first model will be a model with n_estimators, or the number of different data sets, set to 101 and a random state set to 0.  I will continue to adjust my remaining models from there and may run a grid search to find my best number of estimators with the goal of maximizing the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=101, n_jobs=1, oob_score=False, random_state=0,\n",
      "         verbose=0, warm_start=False)\n",
      "Bagging Accuracy 0.8535268937278988\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.89      0.92      0.91      8151\n",
      "        Yes       0.72      0.64      0.68      2595\n",
      "\n",
      "avg / total       0.85      0.85      0.85     10746\n",
      "\n",
      "[[7520  631]\n",
      " [ 943 1652]]\n"
     ]
    }
   ],
   "source": [
    "#Bagging Classifer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf_bag = BaggingClassifier(n_estimators=101, random_state=0) #101 different data sets. Number of estimators needs to be an odd number because your votes could end in a tie.\n",
    "print(clf_bag)\n",
    "clf_bag.fit(features_train, target_train)\n",
    "predicted_bag=clf_bag.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Bagging Accuracy\", accuracy_score(expected,predicted_bag))\n",
    "print(classification_report(expected, predicted_bag,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_bag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.87305225 0.86113657 0.8492209  0.86434464 0.84601283 0.85013749\n",
      " 0.85786337 0.85740486 0.84869326 0.85045872]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8558324875763862"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Random Forest with Cross Validation\n",
    "scores_bc = cross_val_score(clf_bag, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_bc)\n",
    "scores_bc.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=True, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=101, n_jobs=1, oob_score=False, random_state=0,\n",
      "         verbose=0, warm_start=False)\n",
      "Bagging Accuracy 0.861250697934115\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.89      0.93      0.91      8151\n",
      "        Yes       0.75      0.63      0.69      2595\n",
      "\n",
      "avg / total       0.86      0.86      0.86     10746\n",
      "\n",
      "[[7610  541]\n",
      " [ 950 1645]]\n"
     ]
    }
   ],
   "source": [
    "#Bagging Classifer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf_bag = BaggingClassifier(n_estimators=101, random_state=0, bootstrap_features = True)\n",
    "print(clf_bag)\n",
    "clf_bag.fit(features_train, target_train)\n",
    "predicted_bag=clf_bag.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Bagging Accuracy\", accuracy_score(expected,predicted_bag))\n",
    "print(classification_report(expected, predicted_bag,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_bag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.87396884 0.86159487 0.86480293 0.86296975 0.84784601 0.85426214\n",
      " 0.86428244 0.8610729  0.85419532 0.85917431]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.860416952304706"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Random Forest with Cross Validation\n",
    "scores_bc = cross_val_score(clf_bag, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_bc)\n",
    "scores_bc.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=201, n_jobs=4, oob_score=False, random_state=0,\n",
      "         verbose=0, warm_start=True)\n",
      "Bagging Accuracy 0.854178298901917\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.89      0.92      0.91      8151\n",
      "        Yes       0.73      0.63      0.68      2595\n",
      "\n",
      "avg / total       0.85      0.85      0.85     10746\n",
      "\n",
      "[[7537  614]\n",
      " [ 953 1642]]\n"
     ]
    }
   ],
   "source": [
    "#Bagging Classifer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf_bag = BaggingClassifier(n_estimators=201, random_state = 0, n_jobs = 4, warm_start = True)\n",
    "print(clf_bag)\n",
    "clf_bag.fit(features_train, target_train)\n",
    "predicted_bag=clf_bag.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Bagging Accuracy\", accuracy_score(expected,predicted_bag))\n",
    "print(classification_report(expected, predicted_bag,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_bag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.87351054 0.85976169 0.85105408 0.85701192 0.84601283 0.85426214\n",
      " 0.85740486 0.85557084 0.84548372 0.85183486]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.855190748356877"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Random Forest with Cross Validation\n",
    "scores_bc = cross_val_score(clf_bag, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_bc)\n",
    "scores_bc.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search to narrow down n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'n_estimators': 101}\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over several parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"n_estimators\": [77, 101, 151]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf_bag, param_grid=param_grid,n_jobs=-1)\n",
    "grid_search.fit(features_train, target_train)\n",
    "print(\"Best\", grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran 3 separate Bagging Classifier models, all with slightly different parameters tuned.  The first model had n_estimators set to 101 and a random_state of 0 and got an accuracy score of 0.853.  I cross validated this model 10 times to check for overfitting and stayed within 2% on all models.  The second model I ran I added in an additional parameter setting the bootstrap_features to True.  This time my accuracy score increased to 0.861.  I also cross validated 10 times and saw no overfitting.  My final model I adjusted n_estimators to 201, removed the bootstrap_features parameter, set n_jobs to 4 and warm_start to True. This resulted in a slightly worse model with an accuracy score of 0.854.  Cross validating again showed a lack of overfitting.  I ran a grid search as well to try and find my best number of estimators against my second, best performing model, which resulted in my best number still being 101 like I used in my second model. My second Bagging Classifier is my best model for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will next run Support Vector Machine Linear models.  I will run a minimum of 3 models for this section, each with different tuning parameters to try and get a higher accuracy score each time.  The first model will be a model the kernel set to 'linear' since we are running a linear model, C = 1, class_weight = 'balanced', and gamma = 'auto'.  I will continue to adjust my remaining models from there and may run a grid search to find my best C with the goal of maximizing the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Linear Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.94      0.77      0.85      8151\n",
      "        Yes       0.54      0.86      0.66      2595\n",
      "\n",
      "avg / total       0.85      0.79      0.80     10746\n",
      "\n",
      "[[6237 1914]\n",
      " [ 369 2226]]\n",
      "0.7875488553880514\n",
      "Time to run 50.067325344221764 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.clock()\n",
    "from sklearn.svm import SVC\n",
    "#standard linear SVC\n",
    "clf_lin = SVC(kernel='linear', C=1.0, class_weight='balanced',gamma='auto')\n",
    "clf_lin.fit(features_train, target_train)\n",
    "predicted_SVM=clf_lin.predict(features_test)\n",
    "expected = target_test\n",
    "# summarize the fit of the model\n",
    "print(classification_report(expected, predicted_SVM,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_SVM))\n",
    "print(accuracy_score(expected,predicted_SVM))\n",
    "print(\"Time to run\", time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate SVM Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.80109991 0.77956004 0.77910174 0.79651696 0.77406049 0.77726856\n",
      " 0.7826685  0.78954608 0.77624943 0.79954128]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7855612991098141"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify SVM with Cross Validation\n",
    "scores_SVML = cross_val_score(clf_lin, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_SVML)\n",
    "scores_SVML.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Linear Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.94      0.76      0.84      8151\n",
      "        Yes       0.54      0.86      0.66      2595\n",
      "\n",
      "avg / total       0.85      0.79      0.80     10746\n",
      "\n",
      "[[6231 1920]\n",
      " [ 369 2226]]\n",
      "0.7869905080960358\n",
      "Time to run 44.802260426711655 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "clf_lin = SVC(kernel='linear', C=0.5, class_weight='balanced', random_state = 0)\n",
    "clf_lin.fit(features_train, target_train)\n",
    "predicted_SVM=clf_lin.predict(features_test)\n",
    "expected = target_test\n",
    "# summarize the fit of the model\n",
    "print(classification_report(expected, predicted_SVM,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_SVM))\n",
    "print(accuracy_score(expected,predicted_SVM))\n",
    "print(\"Time to run\", time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate SVM Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.8015582  0.77956004 0.77910174 0.79743355 0.77497709 0.77772686\n",
      " 0.78221    0.78954608 0.77624943 0.79908257]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7857445541081429"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify SMV Linear with Cross Validation\n",
    "scores_SVML = cross_val_score(clf_lin, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_SVML)\n",
    "scores_SVML.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Best C Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES {'mean_fit_time': array([38.69393368, 43.45027261, 54.13019481]), 'std_fit_time': array([0.29479063, 4.65361473, 6.66808474]), 'mean_score_time': array([5.33792605, 4.76717529, 4.29064469]), 'std_score_time': array([0.28207349, 0.29737702, 0.72671846]), 'param_C': masked_array(data=[0.01, 0.05, 1],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.01}, {'C': 0.05}, {'C': 1}], 'split0_test_score': array([0.85174152, 0.85426214, 0.85609533]), 'split1_test_score': array([0.84987394, 0.85308274, 0.85491634]), 'split2_test_score': array([0.84208114, 0.84597754, 0.84483154]), 'split3_test_score': array([0.84826954, 0.85079074, 0.85056154]), 'split4_test_score': array([0.84708849, 0.84731774, 0.84869326]), 'mean_test_score': array([0.84781114, 0.8502865 , 0.85101994]), 'std_test_score': array([0.0032641 , 0.00320191, 0.00411986]), 'rank_test_score': array([3, 2, 1]), 'split0_train_score': array([0.84831815, 0.8509541 , 0.85198556]), 'split1_train_score': array([0.84912904, 0.85073344, 0.85119184]), 'split2_train_score': array([0.85033234, 0.85342654, 0.85336924]), 'split3_train_score': array([0.85010314, 0.85130644, 0.85193674]), 'split4_train_score': array([0.8505701 , 0.85194522, 0.85211711]), 'mean_train_score': array([0.84969055, 0.85167315, 0.8521201 ]), 'std_train_score': array([0.00084315, 0.00096778, 0.00070338])}\n",
      "BEST SCORE 0.8510199404079761\n",
      "BEST PARAM {'C': 1}\n",
      "Time to run 231105.3726468205 seconds\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C':[.01,.05,1]} #have to experiment with which C's to use. \n",
    "svr = SVC(kernel='linear')\n",
    "grid_svm = GridSearchCV(svr, parameters,n_jobs=-1, cv=5)\n",
    "grid_svm.fit(features_train, target_train)\n",
    "print(\"SCORES\", grid_svm.cv_results_)\n",
    "print(\"BEST SCORE\", grid_svm.best_score_)\n",
    "print(\"BEST PARAM\", grid_svm.best_params_)\n",
    "print(\"Time to run\", time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES {'mean_fit_time': array([ 64.56446548,  70.53272147,  95.65852785, 102.05991058]), 'std_fit_time': array([2.39984652, 4.96114301, 5.07587043, 4.86419455]), 'mean_score_time': array([4.68372855, 4.65247626, 4.17992029, 4.04845419]), 'std_score_time': array([0.5197321 , 0.40905879, 0.2516956 , 0.82706492]), 'param_C': masked_array(data=[3, 4, 9, 10],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 3}, {'C': 4}, {'C': 9}, {'C': 10}], 'split0_test_score': array([0.85563703, 0.85586618, 0.85586618, 0.85586618]), 'split1_test_score': array([0.85468714, 0.85468714, 0.85468714, 0.85468714]), 'split2_test_score': array([0.84437314, 0.84437314, 0.84437314, 0.84437314]), 'split3_test_score': array([0.85056154, 0.85056154, 0.85056154, 0.85056154]), 'split4_test_score': array([0.84892251, 0.84915177, 0.84892251, 0.84892251]), 'mean_test_score': array([0.85083658, 0.85092826, 0.85088242, 0.85088242]), 'std_test_score': array([0.00408389, 0.0041177 , 0.00413844, 0.00413844]), 'rank_test_score': array([4, 1, 2, 2]), 'split0_train_score': array([0.85204286, 0.85204286, 0.85204286, 0.85198556]), 'split1_train_score': array([0.85107724, 0.85107724, 0.85107724, 0.85113454]), 'split2_train_score': array([0.85348384, 0.85354114, 0.85348384, 0.85342654]), 'split3_train_score': array([0.85199404, 0.85199404, 0.85199404, 0.85205134]), 'split4_train_score': array([0.85217441, 0.85217441, 0.85217441, 0.85211711]), 'mean_train_score': array([0.85215448, 0.85216594, 0.85215448, 0.85214302]), 'std_train_score': array([0.00077023, 0.00079009, 0.00077023, 0.00073462])}\n",
      "BEST SCORE 0.8509282603713042\n",
      "BEST PARAM {'C': 4}\n",
      "Time to run 231771.65274248444 seconds\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C':[3,4,9,10]} #have to experiment with which C's to use.\n",
    "svr = SVC(kernel='linear')\n",
    "grid_svm = GridSearchCV(svr, parameters,n_jobs=-1, cv=5)\n",
    "grid_svm.fit(features_train, target_train)\n",
    "print(\"SCORES\", grid_svm.cv_results_)\n",
    "print(\"BEST SCORE\", grid_svm.best_score_)\n",
    "print(\"BEST PARAM\", grid_svm.best_params_)\n",
    "print(\"Time to run\", time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Linear Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.88      0.94      0.91      8151\n",
      "        Yes       0.75      0.59      0.66      2595\n",
      "\n",
      "avg / total       0.85      0.85      0.85     10746\n",
      "\n",
      "[[7631  520]\n",
      " [1070 1525]]\n",
      "0.852037967615857\n",
      "Time to run 51.91454774473095 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "clf_lin = SVC(kernel='linear', C=1)\n",
    "clf_lin.fit(features_train, target_train)\n",
    "predicted_SVM=clf_lin.predict(features_test)\n",
    "expected = target_test\n",
    "# summarize the fit of the model\n",
    "print(classification_report(expected, predicted_SVM,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_SVM))\n",
    "print(accuracy_score(expected,predicted_SVM))\n",
    "print(\"Time to run\", time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate SVM Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.85701192 0.85655362 0.84738772 0.86113657 0.84372136 0.84647113\n",
      " 0.85098579 0.85098579 0.84135718 0.85825688]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8513867938822065"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify SVM Linear with Cross Validation\n",
    "scores_SVML = cross_val_score(clf_lin, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_SVML)\n",
    "scores_SVML.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran 3 separate Support Vector Machine Linear models, all with slightly different parameters tuned.  The first model had kernel set to 'linear', as all 3 models did, C = 1.0, class_weight = 'balanced', and gamma = 'auto' and got an accuracy score of 0.787.  I cross validated this model 10 times to check for overfitting and stayed within 3% on all models.  The second model I ran I added in an additional parameter setting the random_state to 0, removed the gamma parameter, and changed my C to 0.5.  This time my accuracy score decreased slightly to 0.786.  I also cross validated 10 times and saw no overfitting.  I ran a grid search next to try and find my best C value. I ran 7 different C values through a basic SMV linear model with no additional parameters set and cross validated each 5 times. My best C value turned out to be a 1 with a model accuracy of 0.851. For my final model I adjusted C to 1 and had no other parameters set other than kernel = 'linear'. This resulted in my best accuracy score with my simplest model of 0.852.  Cross validating again showed a lack of overfitting.  My final SVM Linear is my best model for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Accuracy 0.8604131769960915\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.88      0.94      0.91      8151\n",
      "        Yes       0.77      0.61      0.68      2595\n",
      "\n",
      "avg / total       0.85      0.86      0.85     10746\n",
      "\n",
      "[[7671  480]\n",
      " [1020 1575]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf_GBC = GradientBoostingClassifier(n_estimators=100, learning_rate=0.7, max_depth=1, random_state=0)\n",
    "clf_GBC.fit(features_train, target_train)\n",
    "predicted_GBC=clf_GBC.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Gradient Boost Accuracy\", accuracy_score(expected,predicted_GBC))\n",
    "print(classification_report(expected, predicted_GBC,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_GBC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.88038497 0.86892759 0.86342805 0.86571952 0.85747021 0.85472044\n",
      " 0.87116002 0.86336543 0.85602934 0.86834862]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8649554194335053"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Random Forest with Cross Validation\n",
    "scores_gb = cross_val_score(clf_GBC, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_gb)\n",
    "scores_gb.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Accuracy 0.8644146659222036\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.89      0.94      0.91      8151\n",
      "        Yes       0.76      0.64      0.69      2595\n",
      "\n",
      "avg / total       0.86      0.86      0.86     10746\n",
      "\n",
      "[[7629  522]\n",
      " [ 935 1660]]\n"
     ]
    }
   ],
   "source": [
    "clf_GBC = GradientBoostingClassifier(loss = 'exponential', n_estimators=200, learning_rate=0.9, max_depth=1, random_state=0)\n",
    "clf_GBC.fit(features_train, target_train)\n",
    "predicted_GBC=clf_GBC.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Gradient Boost Accuracy\", accuracy_score(expected,predicted_GBC))\n",
    "print(classification_report(expected, predicted_GBC,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_GBC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.88359303 0.87488543 0.86434464 0.87396884 0.85609533 0.85747021\n",
      " 0.87253553 0.86382393 0.85923888 0.86651376]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8672469581059723"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Random Forest with Cross Validation\n",
    "scores_gb = cross_val_score(clf_GBC, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_gb)\n",
    "scores_gb.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Accuracy 0.8568769774799926\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.88      0.94      0.91      8151\n",
      "        Yes       0.75      0.61      0.67      2595\n",
      "\n",
      "avg / total       0.85      0.86      0.85     10746\n",
      "\n",
      "[[7629  522]\n",
      " [1016 1579]]\n"
     ]
    }
   ],
   "source": [
    "clf_GBC = GradientBoostingClassifier(n_estimators=400, learning_rate=1.0, max_depth=1, random_state=0, criterion = 'mse')\n",
    "clf_GBC.fit(features_train, target_train)\n",
    "predicted_GBC=clf_GBC.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Gradient Boost Accuracy\", accuracy_score(expected,predicted_GBC))\n",
    "print(classification_report(expected, predicted_GBC,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_GBC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.86984418 0.85976169 0.86205316 0.86434464 0.85426214 0.86388634\n",
      " 0.86474094 0.85832187 0.84915177 0.86009174]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8606458477574108"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Random Forest with Cross Validation\n",
    "scores_gb = cross_val_score(clf_GBC, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_gb)\n",
    "scores_gb.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search to narrow down n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"n_estimators\": [150, 200, 250]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf_GBC, param_grid=param_grid,n_jobs=-1)\n",
    "grid_search.fit(features_train, target_train)\n",
    "print(\"Best\", grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran 3 separate Gradient Boosting models, plus a gridsearch on the n_estimators, all with slightly different parameters tuned.  The first model had n_estimators set to 100, a random_state of 0, learning_rate of 0.7, and a max_depth of 1 and got an accuracy score of 0.860.  I cross validated this model 10 times to check for overfitting and stayed within 3% on all models.  The second model I ran I added in an additional parameter setting the loss to 'exponential', plus I changed the n_estimators to 200 and the learning_rate to 0.9.  This time my accuracy score increased to 0.864.  I also cross validated 10 times and saw no overfitting.  My final model I adjusted n_estimators to 400, removed the loss parameter, set learning_rate to 1 and criterion to 'mse'. This resulted in a slightly worse model with an accuracy score of 0.856.  Cross validating again showed a lack of overfitting.  I ran a grid search as well to try and find my best number of estimators against my second, best performing model, which resulted in my best number still being 200 like I used in my second model. My second Gradient Boosting model is my best model for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM RBF Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will next run Support Vector Machine RBF models.  I will run a minimum of 3 models for this section, each with different tuning parameters to try and get a higher accuracy score each time.  The first model will be a model the kernel set to 'rbf' since we are running a linear model, C = 1, class_weight = 'balanced', and gamma = 'auto'.  I will continue to adjust my remaining models from there and may run a grid search to find my best C with the goal of maximizing the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM RBF Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.95      0.78      0.86      8151\n",
      "        Yes       0.56      0.87      0.68      2595\n",
      "\n",
      "avg / total       0.85      0.80      0.81     10746\n",
      "\n",
      "[[6371 1780]\n",
      " [ 344 2251]]\n",
      "0.8023450586264657\n",
      "Time to run 2926.8777479404816 seconds\n"
     ]
    }
   ],
   "source": [
    "#standard RBF SVM Model\n",
    "clf_rbf = SVC(kernel='rbf', C=1.0, class_weight='balanced',gamma='auto')\n",
    "clf_rbf.fit(features_train, target_train)\n",
    "predicted_SVM=clf_rbf.predict(features_test)\n",
    "expected = target_test\n",
    "# summarize the fit of the model\n",
    "print(classification_report(expected, predicted_SVM,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_SVM))\n",
    "print(accuracy_score(expected,predicted_SVM))\n",
    "print(\"Time to run\", time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate SVM RBF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.81209899 0.79835014 0.79101742 0.80843263 0.78230981 0.78872594\n",
      " 0.78908757 0.80146722 0.79046309 0.80458716]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7966539959758768"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify SVM with Cross Validation\n",
    "scores_SVMR = cross_val_score(clf_rbf, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_SVMR)\n",
    "scores_SVMR.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM RBF Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.95      0.77      0.85      8151\n",
      "        Yes       0.54      0.87      0.67      2595\n",
      "\n",
      "avg / total       0.85      0.79      0.80     10746\n",
      "\n",
      "[[6248 1903]\n",
      " [ 341 2254]]\n",
      "0.791178112786153\n",
      "Time to run 58.81894648846355 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "clf_rbf = SVC(kernel='rbf', C=0.5, class_weight='balanced', random_state = 0)\n",
    "clf_rbf.fit(features_train, target_train)\n",
    "predicted_SVM=clf_rbf.predict(features_test)\n",
    "expected = target_test\n",
    "# summarize the fit of the model\n",
    "print(classification_report(expected, predicted_SVM,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_SVM))\n",
    "print(accuracy_score(expected,predicted_SVM))\n",
    "print(\"Time to run\", time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate SVM RBF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.80064161 0.78735105 0.77956004 0.79789184 0.7731439  0.77772686\n",
      " 0.77624943 0.79642366 0.77670793 0.79862385]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.786432017815046"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_SVMR = cross_val_score(clf_rbf, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_SVMR)\n",
    "scores_SVMR.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Best C Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES {'mean_fit_time': array([51.66957312, 47.1669734 , 36.21784201]), 'std_fit_time': array([1.32462389, 1.59379833, 2.78810512]), 'mean_score_time': array([7.49796715, 6.53334064, 4.52351556]), 'std_score_time': array([0.34181267, 0.15449999, 0.75941942]), 'param_C': masked_array(data=[0.01, 0.05, 1],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.01}, {'C': 0.05}, {'C': 1}], 'split0_test_score': array([0.78139322, 0.84601283, 0.8588451 ]), 'split1_test_score': array([0.78294751, 0.83795554, 0.85033234]), 'split2_test_score': array([0.78203071, 0.83543433, 0.84437314]), 'split3_test_score': array([0.78271831, 0.83635113, 0.85720834]), 'split4_test_score': array([0.78037597, 0.83677212, 0.84777625]), 'mean_test_score': array([0.78189319, 0.83850562, 0.85170754]), 'std_test_score': array([0.00093411, 0.00384064, 0.00551999]), 'rank_test_score': array([3, 2, 1]), 'split0_train_score': array([0.77961148, 0.83697209, 0.85250129]), 'split1_train_score': array([0.78271831, 0.83892964, 0.85405684]), 'split2_train_score': array([0.78569791, 0.84064864, 0.85634884]), 'split3_train_score': array([0.78162961, 0.84053404, 0.85417144]), 'split4_train_score': array([0.78290265, 0.8400848 , 0.85463817]), 'mean_train_score': array([0.782512  , 0.83943384, 0.85434332]), 'std_train_score': array([0.00197671, 0.0013729 , 0.001234  ])}\n",
      "BEST SCORE 0.8517075406830162\n",
      "BEST PARAM {'C': 1}\n",
      "Time to run 1266.784465787001 seconds\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C':[.01,.05,1]} #have to experiment with which C's to use. \n",
    "svr = SVC(kernel='rbf')\n",
    "grid_svm = GridSearchCV(svr, parameters,n_jobs=-1, cv=5)\n",
    "grid_svm.fit(features_train, target_train)\n",
    "print(\"SCORES\", grid_svm.cv_results_)\n",
    "print(\"BEST SCORE\", grid_svm.best_score_)\n",
    "print(\"BEST PARAM\", grid_svm.best_params_)\n",
    "print(\"Time to run\", time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES {'mean_fit_time': array([41.64142442, 37.3972538 , 37.93918581, 41.21489067]), 'std_fit_time': array([1.52497066, 1.48762975, 0.36958532, 1.51968874]), 'mean_score_time': array([6.04763503, 4.55941014, 4.43094711, 4.37291465]), 'std_score_time': array([0.78420407, 0.12081133, 0.09255344, 0.05092132]), 'param_C': masked_array(data=[3, 4, 9, 10],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 3}, {'C': 4}, {'C': 9}, {'C': 10}], 'split0_test_score': array([0.8627406 , 0.8631989 , 0.86388634, 0.8631989 ]), 'split1_test_score': array([0.85377034, 0.85491634, 0.85422874, 0.85377034]), 'split2_test_score': array([0.84620674, 0.84506074, 0.84804034, 0.84758194]), 'split3_test_score': array([0.85904194, 0.85927114, 0.85858354, 0.85835434]), 'split4_test_score': array([0.8519028 , 0.8523613 , 0.85006878, 0.85006878]), 'mean_test_score': array([0.85473298, 0.85496218, 0.85496218, 0.85459546]), 'std_test_score': array([0.00573298, 0.00618433, 0.0057474 , 0.00563066]), 'rank_test_score': array([3, 1, 1, 4]), 'split0_train_score': array([0.856226  , 0.85708555, 0.85926308, 0.8596069 ]), 'split1_train_score': array([0.85864084, 0.85944304, 0.86190694, 0.86247994]), 'split2_train_score': array([0.86173504, 0.86230804, 0.86437085, 0.86448545]), 'split3_train_score': array([0.85778134, 0.85829704, 0.86225074, 0.86288105]), 'split4_train_score': array([0.85905002, 0.86019595, 0.86311809, 0.86351917]), 'mean_train_score': array([0.85868665, 0.85946593, 0.86218194, 0.8625945 ]), 'std_train_score': array([0.00180561, 0.00176882, 0.00168925, 0.00164022])}\n",
      "BEST SCORE 0.8549621819848728\n",
      "BEST PARAM {'C': 4}\n",
      "Time to run 1743.9262595537293 seconds\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C':[3,4,9,10]} #have to experiment with which C's to use.\n",
    "svr = SVC(kernel='rbf')\n",
    "grid_svm = GridSearchCV(svr, parameters,n_jobs=-1, cv=5)\n",
    "grid_svm.fit(features_train, target_train)\n",
    "print(\"SCORES\", grid_svm.cv_results_)\n",
    "print(\"BEST SCORE\", grid_svm.best_score_)\n",
    "print(\"BEST PARAM\", grid_svm.best_params_)\n",
    "print(\"Time to run\", time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM RBF Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.88      0.94      0.91      8151\n",
      "        Yes       0.76      0.59      0.66      2595\n",
      "\n",
      "avg / total       0.85      0.86      0.85     10746\n",
      "\n",
      "[[7662  489]\n",
      " [1066 1529]]\n",
      "0.8552949934859483\n",
      "Time to run 37.43058285675943 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "clf_rbf = SVC(kernel='rbf', C=4)\n",
    "clf_rbf.fit(features_train, target_train)\n",
    "predicted_SVM=clf_rbf.predict(features_test)\n",
    "expected = target_test\n",
    "# summarize the fit of the model\n",
    "print(classification_report(expected, predicted_SVM,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_SVM))\n",
    "print(accuracy_score(expected,predicted_SVM))\n",
    "print(\"Time to run\", time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate SVM RBF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.86892759 0.86067828 0.85334555 0.85792851 0.84463795 0.84647113\n",
      " 0.86382393 0.85694635 0.84181568 0.86146789]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85560428605635"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_SVMR = cross_val_score(clf_rbf, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_SVMR)\n",
    "scores_SVMR.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran 3 separate Support Vector Machine RBF models, all with slightly different parameters tuned.  The first model had kernel set to 'rbf', as all 3 models did, C = 1.0, class_weight = 'balanced', and gamma = 'auto' and got an accuracy score of 0.802.  I cross validated this model 10 times to check for overfitting and stayed within 2% on all models.  The second model I ran I added in an additional parameter setting the random_state to 0, removed the gamma parameter, and changed my C to 0.5.  This time my accuracy score decreased slightly to 0.791.  I also cross validated 10 times and saw no overfitting.  I ran a grid search next to try and find my best C value. I ran 7 different C values through a basic SMV RBF model with no additional parameters set and cross validated each 5 times. My best C value turned out to be a 4 with a model accuracy of 0.854. For my final model I adjusted C to 4 and had no other parameters set other than kernel = 'rbf'. This resulted in my best accuracy score with my simplest model of 0.855.  Cross validating again showed a lack of overfitting.  My final SVM RBF model is the best for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will next run Extra Trees classification models.  I will run a minimum of 3 models for this section, each with different tuning parameters to try and get a higher accuracy score each time.  The first model will be a basic Extra Trees model with no default parameters tuned, meaning we will have 10 trees.  I will continue to adjust my remaining models from there and may run a grid search on the number of estimators used with the goal of maximizing the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees 0.8257025870091197\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.86      0.91      0.89      8151\n",
      "        Yes       0.67      0.55      0.60      2595\n",
      "\n",
      "avg / total       0.82      0.83      0.82     10746\n",
      "\n",
      "[[7445  706]\n",
      " [1167 1428]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "xdt = ExtraTreesClassifier()\n",
    "xdt.fit(features_train, target_train)\n",
    "predicted_xdt=xdt.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Extra Trees\", accuracy_score(expected,predicted_xdt))\n",
    "print(classification_report(expected, predicted_xdt,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_xdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.83593034 0.82951421 0.83272227 0.82813932 0.82813932 0.83409716\n",
      " 0.82301696 0.81522238 0.82255846 0.82844037]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8277780787595379"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Extra Trees with Cross Validation\n",
    "scores_et = cross_val_score(xdt, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_et)\n",
    "scores_et.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees 0.8299832495812395\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.87      0.91      0.89      8151\n",
      "        Yes       0.67      0.58      0.62      2595\n",
      "\n",
      "avg / total       0.82      0.83      0.83     10746\n",
      "\n",
      "[[7408  743]\n",
      " [1084 1511]]\n"
     ]
    }
   ],
   "source": [
    "xdt = ExtraTreesClassifier(criterion = 'entropy',n_estimators=40, warm_start = True, class_weight = 'balanced_subsample')\n",
    "xdt.fit(features_train, target_train)\n",
    "predicted_xdt=xdt.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Extra Trees\", accuracy_score(expected,predicted_xdt))\n",
    "print(classification_report(expected, predicted_xdt,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_xdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.83913841 0.83363886 0.83226398 0.83547204 0.82722273 0.83226398\n",
      " 0.83218707 0.82072444 0.82393398 0.83669725]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8313542731429486"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Extra Trees with Cross Validation\n",
    "scores_et = cross_val_score(xdt, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_et)\n",
    "scores_et.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees 0.8344500279173646\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.88      0.91      0.89      8151\n",
      "        Yes       0.68      0.60      0.63      2595\n",
      "\n",
      "avg / total       0.83      0.83      0.83     10746\n",
      "\n",
      "[[7422  729]\n",
      " [1050 1545]]\n"
     ]
    }
   ],
   "source": [
    "xdt = ExtraTreesClassifier(criterion = 'entropy', n_estimators=100, class_weight = 'balanced', n_jobs = 4)\n",
    "xdt.fit(features_train, target_train)\n",
    "predicted_xdt=xdt.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Extra Trees\", accuracy_score(expected,predicted_xdt))\n",
    "print(classification_report(expected, predicted_xdt,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_xdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.84463795 0.83363886 0.83501375 0.83776352 0.83088909 0.83776352\n",
      " 0.83127006 0.82164145 0.8262265  0.83394495]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8332789655324632"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Extra Trees with Cross Validation\n",
    "scores_et = cross_val_score(xdt, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_et)\n",
    "scores_et.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search to narrow down n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best {'n_estimators': 70}\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over several parameters\n",
    "param_grid = {\"n_estimators\": [70, 100, 150]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(xdt, param_grid=param_grid,n_jobs=-1)\n",
    "grid_search.fit(features_train, target_train)\n",
    "print(\"Best\", grid_search.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees Model - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees 0.8330541596873255\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.87      0.91      0.89      8151\n",
      "        Yes       0.68      0.59      0.63      2595\n",
      "\n",
      "avg / total       0.83      0.83      0.83     10746\n",
      "\n",
      "[[7419  732]\n",
      " [1062 1533]]\n"
     ]
    }
   ],
   "source": [
    "xdt = ExtraTreesClassifier(criterion = 'entropy', n_estimators=70, class_weight = 'balanced', n_jobs = 4)\n",
    "xdt.fit(features_train, target_train)\n",
    "predicted_xdt=xdt.predict(features_test)\n",
    "expected = target_test\n",
    "print(\"Extra Trees\", accuracy_score(expected,predicted_xdt))\n",
    "print(classification_report(expected, predicted_xdt,target_names=['No', 'Yes']))\n",
    "print(confusion_matrix(expected, predicted_xdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score for each K [0.84463795 0.83363886 0.83501375 0.83776352 0.83088909 0.83776352\n",
      " 0.83127006 0.82164145 0.8262265  0.83394495]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8332789655324632"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify Extra Trees with Cross Validation\n",
    "scores_et = cross_val_score(xdt, features_train, target_train, cv=10)  \n",
    "print(\"Cross Validation Score for each K\",scores_et)\n",
    "scores_et.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran 4 separate Extra Tress models, all with slightly different parameters tuned.  The first model I ran was the base model got an accuracy score of 0.825.  I cross validated this model 10 times to check for overfitting and stayed within 2% on all models.  The second model I ran I adjusted the criterion to 'entropy', n_estimators to 40 from the default of 10, set warm_start to True, and set the class_weight to 'balanced_subsample'.  This time my accuracy score increased to 0.829.  I also cross validated 10 times and saw no overfitting.  My third model I adjusted n_estimators to 100, kept criterion equal to 'entropy' and class_weight equal to 'balanced', and now adjusted n_jobs to 4, while removing the class_weight parameter.  This resulted in a slightly better model with an accuracy score of 0.834.  Cross validating again showed a lack of overfitting.  Finally, I ran a grid search to find my best n_estimators number, which turned out to be 70.  I ran one final model, similar to model 3 but with n_estimators at 70, and got an accuracy score of 0.833. My third Extra Trees Model is my best model for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will next run ANN classification models.  I will run a minimum of 3 models for this section, each with different tuning parameters to try and get a higher accuracy score each time.  The first model will be a basic MLPClassifier model with no default parameters tuned.  I will continue to adjust my remaining models from there with the goal of maximizing the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.85250325702587\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Salaray <= 50k       0.90      0.91      0.90      8151\n",
      " Salary >= 50k       0.70      0.67      0.69      2595\n",
      "\n",
      "   avg / total       0.85      0.85      0.85     10746\n",
      "\n",
      "[[7420  731]\n",
      " [ 854 1741]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf_ann =MLPClassifier()\n",
    "clf_ann.fit(features_train, target_train)\n",
    "target_predicted_ann = clf_ann.predict(features_test)\n",
    "print(\"Accuracy\", accuracy_score(target_test, target_predicted_ann))\n",
    "target_names = [\"Salaray <= 50k\", \"Salary >= 50k\"]\n",
    "print(classification_report(target_test, target_predicted_ann, target_names=target_names))\n",
    "print(confusion_matrix(target_test, target_predicted_ann))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8575283826540108\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Salaray <= 50k       0.89      0.93      0.91      8151\n",
      " Salary >= 50k       0.74      0.63      0.68      2595\n",
      "\n",
      "   avg / total       0.85      0.86      0.85     10746\n",
      "\n",
      "[[7579  572]\n",
      " [ 959 1636]]\n"
     ]
    }
   ],
   "source": [
    "clf_ann =MLPClassifier(hidden_layer_sizes=(10,10,10), max_iter=500, alpha=1e-05,\n",
    "                     random_state=21,tol=0.000000001)\n",
    "clf_ann.fit(features_train, target_train)\n",
    "target_predicted_ann = clf_ann.predict(features_test)\n",
    "print(\"Accuracy\", accuracy_score(target_test, target_predicted_ann))\n",
    "target_names = [\"Salaray <= 50k\", \"Salary >= 50k\"]\n",
    "print(classification_report(target_test, target_predicted_ann, target_names=target_names))\n",
    "print(confusion_matrix(target_test, target_predicted_ann))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8580867299460264\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Salaray <= 50k       0.88      0.94      0.91      8151\n",
      " Salary >= 50k       0.75      0.61      0.68      2595\n",
      "\n",
      "   avg / total       0.85      0.86      0.85     10746\n",
      "\n",
      "[[7627  524]\n",
      " [1001 1594]]\n"
     ]
    }
   ],
   "source": [
    "clf_ann =MLPClassifier(hidden_layer_sizes=(5,5,5,5), max_iter=500, alpha=1e-05,\n",
    "                     random_state=21,tol=0.000000001)\n",
    "clf_ann.fit(features_train, target_train)\n",
    "target_predicted_ann = clf_ann.predict(features_test)\n",
    "print(\"Accuracy\", accuracy_score(target_test, target_predicted_ann))\n",
    "target_names = [\"Salaray <= 50k\", \"Salary >= 50k\"]\n",
    "print(classification_report(target_test, target_predicted_ann, target_names=target_names))\n",
    "print(confusion_matrix(target_test, target_predicted_ann))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Model - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8577144984180161\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Salaray <= 50k       0.89      0.93      0.91      8151\n",
      " Salary >= 50k       0.74      0.64      0.68      2595\n",
      "\n",
      "   avg / total       0.85      0.86      0.85     10746\n",
      "\n",
      "[[7569  582]\n",
      " [ 947 1648]]\n"
     ]
    }
   ],
   "source": [
    "clf_ann =MLPClassifier(hidden_layer_sizes=(5,5,5,5), max_iter=200, alpha=0.4,\n",
    "                     random_state=21,tol=0.000000001)\n",
    "clf_ann.fit(features_train, target_train)\n",
    "target_predicted_ann = clf_ann.predict(features_test)\n",
    "print(\"Accuracy\", accuracy_score(target_test, target_predicted_ann))\n",
    "target_names = [\"Salaray <= 50k\", \"Salary >= 50k\"]\n",
    "print(classification_report(target_test, target_predicted_ann, target_names=target_names))\n",
    "print(confusion_matrix(target_test, target_predicted_ann))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Model - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8324027545133073\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Salaray <= 50k       0.88      0.90      0.89      8151\n",
      " Salary >= 50k       0.66      0.62      0.64      2595\n",
      "\n",
      "   avg / total       0.83      0.83      0.83     10746\n",
      "\n",
      "[[7337  814]\n",
      " [ 987 1608]]\n"
     ]
    }
   ],
   "source": [
    "clf_ann =MLPClassifier(hidden_layer_sizes=(100,100), max_iter=500, alpha=1e-05,\n",
    "                     random_state=21,tol=0.000000001)\n",
    "clf_ann.fit(features_train, target_train)\n",
    "target_predicted_ann = clf_ann.predict(features_test)\n",
    "print(\"Accuracy\", accuracy_score(target_test, target_predicted_ann))\n",
    "target_names = [\"Salaray <= 50k\", \"Salary >= 50k\"]\n",
    "print(classification_report(target_test, target_predicted_ann, target_names=target_names))\n",
    "print(confusion_matrix(target_test, target_predicted_ann))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Model - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8384515168434766\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Salaray <= 50k       0.90      0.89      0.89      8151\n",
      " Salary >= 50k       0.66      0.68      0.67      2595\n",
      "\n",
      "   avg / total       0.84      0.84      0.84     10746\n",
      "\n",
      "[[7243  908]\n",
      " [ 828 1767]]\n"
     ]
    }
   ],
   "source": [
    "clf_ann =MLPClassifier(hidden_layer_sizes=(20,20,20,20), max_iter=200, alpha=1e-05,\n",
    "                     random_state=21,tol=0.000000001)\n",
    "clf_ann.fit(features_train, target_train)\n",
    "# test random forest model\n",
    "target_predicted_ann = clf_ann.predict(features_test)\n",
    "print(\"Accuracy\", accuracy_score(target_test, target_predicted_ann))\n",
    "target_names = [\"Salaray <= 50k\", \"Salary >= 50k\"]\n",
    "print(classification_report(target_test, target_predicted_ann, target_names=target_names))\n",
    "print(confusion_matrix(target_test, target_predicted_ann))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran 6 separate ANN, all with slightly different parameters tuned.  The first model I ran was the base model got an accuracy score of 0.852.  The second model I ran I adjusted my hidden layers to 10,10,10, my max_iter to 500, my alpha to 1e-05, random_state to 21, and tol to 0.000000001.  This time my accuracy score increased to 0.857.  My third model I adjusted my hidden layers to 5,5,5,5 and kept all other parameters the same as model 2.  This resulted in a slightly better model with an accuracy score of 0.858.  My fourth model I kept my hidden layers at 5,5,5,5 and adjusted my max_iter to 200 and alpha to 0.4 while the random state and tol remained the same.  This resulted in an accuracy score of 0.857.  My fifth model I adjusted the hidden layers to 100,100, max_iter to 500, put the alpha back to 1e-05, and kept random_state and tol the same.  This gave me an accuracy score of 0.832. Finally, I ran a model with hidden layers of 20,20,20,20, max_iter of 200, and kept my alpha, random_state, and tol the same as model 5. This gave me an accuracy score of 0.838. My third ANN is my best model for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will next run Stacking models.  I will run a minimum of 3 models for this section, each with different tuning parameters to try and get a higher accuracy score each time.  The first model will my top 3 models in terms of accuracy score that I have developed thus far.  I will continue to adjust my remaining models from there with the goal of maximizing the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867751 (+/- 0.01) [Gradient Boosting Classifier]\n",
      "Accuracy: 0.858675 (+/- 0.00) [Bagging Classifier]\n",
      "Accuracy: 0.856153 (+/- 0.01) [ANN]\n",
      "Accuracy: 0.866238 (+/- 0.01) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#Three Models Gradient Boosting, Bagging Classifier, ANN\n",
    "clf1 = GradientBoostingClassifier(loss = 'exponential', n_estimators=200, learning_rate=0.9, max_depth=1, random_state=0)\n",
    "clf2 = BaggingClassifier(n_estimators=101, random_state=0, bootstrap_features = True)\n",
    "clf3 = MLPClassifier(hidden_layer_sizes=(5,5,5,5), max_iter=500, alpha=1e-05,\n",
    "                     random_state=21,tol=0.000000001)\n",
    "eclf2 = VotingClassifier(estimators=[('gb', clf1), ('bc', clf2), ('ann', clf3)], voting='hard')\n",
    "for MV, label in zip([clf1, clf2, clf3, eclf2], ['Gradient Boosting Classifier', 'Bagging Classifier', 'ANN', 'Ensemble']):\n",
    "\n",
    "    scores2 = cross_val_score(MV, features_train, target_train, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.6f (+/- %0.2f) [%s]\" % (scores2.mean(), scores2.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867201 (+/- 0.01) [Gradient Boosting Classifier]\n",
      "Accuracy: 0.849323 (+/- 0.00) [Bagging Classifier]\n",
      "Accuracy: 0.848407 (+/- 0.00) [ANN]\n",
      "Accuracy: 0.864313 (+/- 0.00) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "#Three Models Gradient Boosting, Bagging Classifier, ANN\n",
    "clf1 = GradientBoostingClassifier()\n",
    "clf2 = BaggingClassifier()\n",
    "clf3 = MLPClassifier()\n",
    "eclf2 = VotingClassifier(estimators=[('gb', clf1), ('bc', clf2), ('ann', clf3)], voting='hard')\n",
    "for MV, label in zip([clf1, clf2, clf3, eclf2], ['Gradient Boosting Classifier', 'Bagging Classifier', 'ANN', 'Ensemble']):\n",
    "\n",
    "    scores2 = cross_val_score(MV, features_train, target_train, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.6f (+/- %0.2f) [%s]\" % (scores2.mean(), scores2.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.831125 (+/- 0.01) [KNN]\n",
      "Accuracy: 0.813247 (+/- 0.01) [Decision Tree]\n",
      "Accuracy: 0.847948 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.827733 (+/- 0.01) [Extra Trees]\n",
      "Accuracy: 0.843639 (+/- 0.00) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "#Three Models KNN, Decision Tree, Random Forest, Extra Trees\n",
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = tree.DecisionTreeClassifier()\n",
    "clf3 = RandomForestClassifier()\n",
    "clf4 = ExtraTreesClassifier()\n",
    "eclf2 = VotingClassifier(estimators=[('knn', clf1), ('dt', clf2), ('rf', clf3), ('et', clf4)], voting='hard')\n",
    "for MV, label in zip([clf1, clf2, clf3, clf4, eclf2], ['KNN', 'Decision Tree', 'Random Forest', 'Extra Trees', 'Ensemble']):\n",
    "\n",
    "    scores2 = cross_val_score(MV, features_train, target_train, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.6f (+/- %0.2f) [%s]\" % (scores2.mean(), scores2.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Model - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.837359 (+/- 0.01) [KNN]\n",
      "Accuracy: 0.819160 (+/- 0.01) [Decision Tree]\n",
      "Accuracy: 0.854641 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.832684 (+/- 0.01) [Extra Trees]\n",
      "Accuracy: 0.847949 (+/- 0.00) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "#Three Models KNN, Decision Tree, Random Forest, Extra Trees\n",
    "clf1 = KNeighborsClassifier(n_neighbors = 15, p = 1, weights = 'distance')\n",
    "clf2 = tree.DecisionTreeClassifier(criterion = 'gini', min_samples_split = 5)\n",
    "clf3 = RandomForestClassifier(n_estimators= 500, n_jobs=4,oob_score=True, criterion = 'entropy')\n",
    "clf4 = ExtraTreesClassifier(criterion = 'entropy', n_estimators=100, class_weight = 'balanced', n_jobs = 4)\n",
    "eclf2 = VotingClassifier(estimators=[('knn', clf1), ('dt', clf2), ('rf', clf3), ('et', clf4)], voting='hard')\n",
    "for MV, label in zip([clf1, clf2, clf3, clf4, eclf2], ['KNN', 'Decision Tree', 'Random Forest', 'Extra Trees', 'Ensemble']):\n",
    "\n",
    "    scores2 = cross_val_score(MV, features_train, target_train, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.6f (+/- %0.2f) [%s]\" % (scores2.mean(), scores2.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran 4 separate Stacking models, all with slightly different parameters tuned.  The first model I ran was used my top 3 models from the above analysis, including Gradient Boosting 1 and 2 and the Bagging Classifier 2 which gave me an accuracy score of 0.866.  My second stacking model I used the base Gradient Boosting and Bagging Classifier models to try and simplify the model and got an accuracy score of 0.864.  My third model used the base KNN, Decision Tree, Random Forest, and Extra Trees models and got an accuracy score of 0.843.  My final model used those same four models but with parameters tuned within them that were the same as the third model I ran for each of them above, getting an accuracy score of 0.847.  My first stacking model is my best model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
